{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tika\n",
      "  Downloading https://files.pythonhosted.org/packages/96/07/244fbb9c74c0de8a3745cc9f3f496077a29f6418c7cbd90d68fd799574cb/tika-1.24.tar.gz\n",
      "Requirement already satisfied: setuptools in c:\\users\\gabe5\\anaconda3\\lib\\site-packages (from tika) (41.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\gabe5\\anaconda3\\lib\\site-packages (from tika) (2.22.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\gabe5\\anaconda3\\lib\\site-packages (from requests->tika) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\gabe5\\anaconda3\\lib\\site-packages (from requests->tika) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\gabe5\\anaconda3\\lib\\site-packages (from requests->tika) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gabe5\\anaconda3\\lib\\site-packages (from requests->tika) (2019.11.28)\n",
      "Building wheels for collected packages: tika\n",
      "  Building wheel for tika (setup.py): started\n",
      "  Building wheel for tika (setup.py): finished with status 'done'\n",
      "  Created wheel for tika: filename=tika-1.24-cp37-none-any.whl size=32887 sha256=a72ae011cf8e1c50094090bb9aa57c1f23edd8ab17f4efd596590778a420639d\n",
      "  Stored in directory: C:\\Users\\gabe5\\AppData\\Local\\pip\\Cache\\wheels\\73\\9c\\f5\\0b1b738442fc2a2862bef95b908b374f8e80215550fb2a8975\n",
      "Successfully built tika\n",
      "Installing collected packages: tika\n",
      "Successfully installed tika-1.24\n"
     ]
    }
   ],
   "source": [
    "!pip install tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-11 21:32:59,617 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'E:\\Documents\\Books\\How emotions are made.pdf'\n",
    "text = parser.from_file(path)\n",
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1185260"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_text = text['content']\n",
    "len(book_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "831286"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_text = book_text[11465:1185260 - 342509 ]\n",
    "len(book_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(book_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2078215.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only about 70% of the text is text useful text from the book\n",
    "831286/1185260\n",
    "8312860 * .25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(path, file = False):\n",
    "    ''' Function to load data file '''\n",
    "    \n",
    "    with open(text,'rb') as file:\n",
    "        data = file.read()\n",
    "        return data\n",
    "    \n",
    "def split_data(data, size = .25, section = 'front', verbose = True):\n",
    "    ''' returns a sample of the data \n",
    "    \n",
    "        parameters\n",
    "        -----------------------\n",
    "        \n",
    "        size: \n",
    "            this determines how much of the data to return ranging from 0-1\n",
    "            \n",
    "        section:\n",
    "            specifies where to grab text from\n",
    "                * front: starts from the begining\n",
    "                * back: grabs from the back\n",
    "                * random: its random \n",
    "                \n",
    "    '''\n",
    "    \n",
    "    if size > 1 or size < 0:\n",
    "        raise ValueError(\"Size must be a value between 1 and 0\")\n",
    "        \n",
    "    amount = int(len(data) * size)\n",
    "    section = section.lower()\n",
    "    if section == 'front':\n",
    "        split = data[:amount]\n",
    "    elif section == 'back':\n",
    "        # use negatives to go from the back\n",
    "        split = data[-amount:-1]\n",
    "    elif section == 'random':\n",
    "        print(\"this isnt ready yet :(\")\n",
    "        pass\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Size: {int(size*100)}% or {len(split)} words')\n",
    "        print(f\"section: {section}\")\n",
    "    \n",
    "    # remove newline characters\n",
    "    split = split.replace(\"\\n\",\"\")\n",
    "    return split\n",
    "\n",
    "def clean(doc, verbose = True):\n",
    "    import string\n",
    "    # replace '--' with \" \"\n",
    "    doc = doc.replace('--',\" \")\n",
    "    # split into a list (tokens)by whitespace\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "    tokens = [word.translate(table) for word in tokens]\n",
    "    # remove tokens that arent alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make all lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Total Characters: {len(tokens)}')\n",
    "        print(f'Unique Characters: {len(sorted(set(tokens)))}')\n",
    "        \n",
    "    return tokens\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the text\n",
    "We need to transform the raw text into a sequence of tokens or words that we can use as a source to train the model.\n",
    "\n",
    "* Replace ‘–‘ with a white space so we can split words better.\n",
    "* Split words based on white space.\n",
    "* Remove all punctuation from words to reduce the vocabulary size (e.g. ‘What?’ becomes ‘What’).\n",
    "* Remove all words that are not alphabetic to remove standalone punctuation tokens.\n",
    "* Normalize all words to lowercase to reduce the vocabulary size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 100% or 831286 words\n",
      "section: front\n",
      "Total Characters: 123945\n",
      "Unique Characters: 10248\n",
      "['introduction', 'the', 'twothousandyearold', 'assumptionon', 'december', 'the', 'deadliest', 'school', 'shooting', 'in', 'us', 'history', 'took', 'place', 'at', 'sandy', 'hook', 'elementary', 'school', 'in', 'newtown', 'connecticut', 'twentysix', 'people', 'inside', 'the', 'school', 'including', 'twenty', 'children', 'were', 'massacred', 'by', 'a', 'lone', 'gunman', 'several', 'weeks', 'after', 'this', 'horror', 'i', 'watched', 'the', 'governor', 'of', 'connecticut', 'dannel', 'malloy', 'give', 'his', 'annual', 'of', 'the', 'speech', 'on', 'television', 'he', 'spoke', 'in', 'a', 'strong', 'and', 'animated', 'voice', 'for', 'the', 'first', 'three', 'minutes', 'thanking', 'individuals', 'for', 'their', 'service', 'and', 'then', 'he', 'began', 'to', 'address', 'the', 'newtown', 'tragedywe', 'have', 'all', 'walked', 'a', 'very', 'long', 'and', 'very', 'dark', 'road', 'together', 'what', 'befell', 'newtown', 'is', 'not', 'something', 'we', 'thought', 'possible', 'in', 'any', 'of', 'beautiful', 'towns', 'or', 'cities', 'and', 'yet', 'in', 'the', 'midst', 'of', 'one', 'of', 'the', 'worst', 'days', 'in', 'our', 'history', 'we', 'also', 'saw', 'the', 'best', 'of', 'our', 'state', 'teachers', 'and', 'a', 'therapist', 'that', 'sacrificed', 'their', 'lives', 'protecting', 'as', 'the', 'governor', 'spoke', 'the', 'last', 'two', 'words']\n"
     ]
    }
   ],
   "source": [
    "doc = split_data(book_text,size = 1)\n",
    "tokens = clean(doc)\n",
    "# print(tokens[:150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create sequences\n",
    "We can organize the long list of tokens into sequences of 50 input words and 1 output word.\n",
    "That is, sequences of 51 words.\n",
    "We can do this by iterating over the list of tokens from token 51 onwards and taking the prior 50 tokens as a sequence, then repeating this process to the end of the list of tokens.\n",
    "We will transform the tokens into space-separated strings for later storage in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "\n",
    "# generate sequences\n",
    "# grabs a subset of the text 51 characters at a time\n",
    "for index in range(length, len(tokens)):\n",
    "    # select a sequence of tokens from 0 to 51, then 52 - 103, and so on\n",
    "    seq = tokens[index - length: index]\n",
    "    #print(index - length,index)\n",
    "    \n",
    "    # flatten the sequence (convert into a string)\n",
    "    line = ' '.join(seq)\n",
    "    # append the sequnce\n",
    "    sequences.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences: 123894\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total sequences: {len(sequences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the sequences\n",
    "Now we can save our sequences to a file so that we can load it in later. Im using a pickle file so that i can load the data back in as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import os\n",
    "def save_seq(seqs,filename,location: str):\n",
    "    # join the location and the filename\n",
    "    path = os.path.join(location,filename)\n",
    "    with open(path,'wb') as file:\n",
    "        pk.dump(seqs, file)\n",
    "        print(f'Sequences saved to: {path}')\n",
    "\n",
    "        \n",
    "    # replace .seq with .txt  \n",
    "    filename = filename.replace(filename[-4:],'.txt')\n",
    "    print(filename)\n",
    "    path = os.path.join(location,filename)\n",
    "    with open(path,'w',encoding = 'UTF-8') as file:\n",
    "        data = '\\n'.join(seqs)\n",
    "        file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences saved to: E:\\Documents\\My Projects\\Text Generation\\data\\HEAM.seq\n",
      "HEAM.txt\n"
     ]
    }
   ],
   "source": [
    "location = r'E:\\Documents\\My Projects\\Text Generation\\data'\n",
    "filename = 'HEAM.seq'\n",
    "save_seq(sequences,filename,location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
