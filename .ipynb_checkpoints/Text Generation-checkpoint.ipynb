{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.92 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np \n",
    "import os\n",
    "import pickle as pk\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    ''' class to load models, encoders, and other methods that help in making predictions '''\n",
    "    def __init__(self):\n",
    "        self.encoder = None\n",
    "        self.model = None\n",
    "        self.sequences = None\n",
    "        self.seq_length = None\n",
    "        self.seed_text = None\n",
    "        self.encoded_seed = None\n",
    "    \n",
    "    def load_sequences(self,path, binary = True):\n",
    "        ''' function that is used to load the text sequences from either\n",
    "        a binary pickle file or a text file. returns a list of sequences \n",
    "        and the expected sequence length'''\n",
    "    \n",
    "        # checks if the file is serialized or not\n",
    "        if binary:\n",
    "            with open(path,'rb') as file:\n",
    "                # loads the sequences \n",
    "                self.sequences = pk.load(file)\n",
    "                print(f'Sequences loaded from: {path}')\n",
    "                \n",
    "        # loads the file from an unserialized format\n",
    "        else:\n",
    "            with open(path,'r') as file:\n",
    "                doc = file.read()\n",
    "                self.sequences = doc.split('\\n')\n",
    "\n",
    "        # seq_len is a vector of size 50\n",
    "        self.seq_length = len(self.sequences[0].split()) - 1\n",
    "        return self.sequences, self.seq_length\n",
    "    \n",
    "    def load_encoder(self,path):\n",
    "        with open(path,'rb') as file:\n",
    "            self.encoder = pk.load(file)\n",
    "            print(f'Encoder loaded from: {path}')\n",
    "\n",
    "        return self.encoder\n",
    "    \n",
    "    def load_network(self,path):\n",
    "        self.model = load_model(path)\n",
    "        return self.model\n",
    "    \n",
    "    def generate_seed(self,sequences = None):\n",
    "        sequences = self.sequences if sequences is None else sequences\n",
    "        section = randint(0,len(sequences[0]))\n",
    "        self.seed_text = self.sequences[section]\n",
    "        print(f'Generated from section: {section}')\n",
    "        return self.seed_text\n",
    "    \n",
    "    def pad_input_sequence(self,seed = None):\n",
    "        # the seed text must be encoded to integers using \n",
    "        # the same tokenizer that we used when training the model.\n",
    "        if self.encoder is None:\n",
    "            raise TypeError(f'Encoder can not be of type: {self.encoder}')\n",
    "        \n",
    "        # load the input sequnce \n",
    "        seed = self.seed_text if seed is None else seed\n",
    "        self.encoded_seed = self.encoder.texts_to_sequences([seed])[0]\n",
    "        # Truncate the sequence to a fixed length \n",
    "        self.encoded_seed = pad_sequences([self.encoded_seed], maxlen = self.seq_length, truncating='pre')\n",
    "        return self.encoded_seed\n",
    "    \n",
    "    def generate_sequence(self, model = None, seed = None, seq_len = None, output_len = 100):\n",
    "        \n",
    "        # all of the input values are set to none by default so the first step is to hanlde this\n",
    "        model = self.model if model is None else model\n",
    "        seq_len = self.seq_length if seq_len is None else seq_len\n",
    "        \n",
    "        \n",
    "        # the list that the output sequence will be loaded into\n",
    "        result = list()\n",
    "        input_text = self.generate_seed() if seed is None else seed\n",
    "        \n",
    "        # generate a fixed number of words\n",
    "        for _ in range(output_len):\n",
    "            # encode the text as integer\n",
    "            encoded = self.pad_input_sequence(input_text)\n",
    "            \n",
    "            # predict probabilities for each word\n",
    "            pred = self.model.predict_classes(encoded, verbose = 0)\n",
    "            \n",
    "            # map predicted word index to word\n",
    "            predicted_word = ''\n",
    "            for word, index in self.encoder.word_index.items():\n",
    "                # check to see if the current index is the index of the predicted word\n",
    "                if index == pred:\n",
    "                    predicted_word = word\n",
    "                    break\n",
    "                    \n",
    "            # append to the input text (this is so that our next predicted word is based on the word we just predicted) +=\n",
    "            input_text += ' ' + predicted_word\n",
    "            result.append(predicted_word) # this list will be our newly generated sequence\n",
    "        \n",
    "        return ' '.join(result)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabe5\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder loaded from: E:\\Documents\\My Projects\\Text Generation\\Models\\encoder.pkl\n",
      "Sequences loaded from: E:\\Documents\\My Projects\\Text Generation\\data\\HEAM.seq\n"
     ]
    }
   ],
   "source": [
    "seq_path = r'E:\\Documents\\My Projects\\Text Generation\\data\\HEAM.seq'\n",
    "encoder_path = r'E:\\Documents\\My Projects\\Text Generation\\Models\\encoder.pkl'\n",
    "model_path = r'E:\\Documents\\My Projects\\Text Generation\\Models\\BiLSTM_Language_Generation.hdf5'\n",
    "\n",
    "# instantiate a predictor class\n",
    "p = Predictor()\n",
    "p.load_network(model_path)\n",
    "p.load_encoder(encoder_path)\n",
    "seq = p.load_sequences(seq_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated from section: 112847\n",
      "290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'of the hunt for the second inevitability of the mind is that you have concepts because the human brain is wired to construct a conceptual system you build concepts for the smallest physical details like fleeting bits of light and sound and for incredibly complex ideas like and not to bring'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = p.generate_seed(sequences=seq)\n",
    "print(len(seed))\n",
    "seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = p.generate_sequence(seed = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your boss and deep out of wavelengths but usually they can benefit in the circumstances you have a banana to his friend you want to get worse but if we try to mention greater intelligent of emotion people who showed them of animals alone like ideas when you see what he gave it or if someone else i am correct angry so not wrong feel them or danger is rendered emotionally longterm learn an emotional creatures who encourage us to learn goalbased concepts like is an instance of anger for someone just like you might use a question for about\n"
     ]
    }
   ],
   "source": [
    "print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
