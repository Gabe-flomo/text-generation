
intelligence is better than humans at playing chess or go but still has trouble holding a conversation or driving 
car a simple way to think about the discrepancy is through the lens of there are features of the 
from the fact that tables are solid to the prediction that a tree walk across the street that humans 
for granted but that machines have difficulty learning melanie mitchell is a computer scientist and complexity researcher who has 
a new book about the prospects of modern ai we talk about deep learning and other ai strategies why 
currently fall short at equipping computers with a functional understanding of the world and how we might move forward 
mindscape on patreon melanie mitchell received her phd in computer science from the university of michigan she is currently 
professor of computer science at portland state university and an external professor at the santa fe institute her research 
on genetic algorithms cellular automata and analogical reasoning she is the author of an introduction to genetic algorithms complexity 
guided tour and most recently artificial intelligence a guide for thinking humans she originated the santa fe complexity explorer 
on online learning resource for complex systems web sitesanta fe web pagewikipediagoogle scholarcomplexity exploreramazoncom author pagetwitter click to show 
transcript click above to close artificial intelligence is better than humans at playing chess or go but still has 
holding a conversation or driving a car a simple way to think about the discrepancy is through the lens 
there are features of the world from the fact that tables are solid to the prediction that a tree 
across the street that humans take for granted but that machines have difficulty learning melanie mitchell is a computer 
and complexity researcher who has written a new book about the prospects of modern ai we talk about deep 
and other ai strategies why they currently fall short at equipping computers with a functional understanding of the world 
how we might move forward support mindscape on patreon melanie mitchell received her phd in computer science from the 
of michigan she is currently a professor of computer science at portland state university and an external professor at 
santa fe institute her research focuses on genetic algorithms cellular automata and analogical reasoning she is the author of 
introduction to genetic algorithms complexity a guided tour and most recently artificial intelligence a guide for thinking humans she 
the santa fe complexity explorer project on online learning resource for complex systems click to show episode transcript sean 
hello everyone and welcome to the mindscape podcast your host sean carroll we all know that artificial intelligence is 
important thing talked about it here on mindscape sure gonna keep talking about it down the road we also 
that it faces some obstacles i mean the kinds of ethical obstacles talked about before what should your selfdriving 
do i mean obstacles in creating what would truly be considered a human level artificial intelligence there are certain 
that computers that ais are really really good at way better than any human being and certain other things 
still fall short at and those things that artificial intelligence is not very good at can be lumped under 
general heading of common sense there are aspects of the world that seem completely obvious and intuitive to every 
being from a very young age and yet not only do computers not know them automatically we even have 
teaching them to computers sc so guest is melanie mitchell a computer scientist and complexity theorist at portland state 
and the santa fe institute the author of several books that i really like and that her new book 
called artificial intelligence a guide for thinking humans and it pretend to give the once and for all solution 
this problem but it lays out exactly this issue of how artificial intelligence works and why common sense as 
and as basic as it is is one of the most difficult things to teach to a computer we 
talk about what the ramifications of this difficulty are for what ai is where going how it will be 
and bad as we move into the future so music sc melanie mitchell thanks for being on the mindscape 
melanie mitchell glad to be here sc so you have an advantage over other people who write popular books 
artificial intelligence i think which is that douglas hofstadter was your phd thesis advisor right mm right right yes 
so that not only is awesome because done a lot of influential things most obviously author of gödel escher 
which probably like you got a lot of people interested in this subject matter but he continues to be 
thoughtful and eloquent person thinking about these issues and so you tell this wonderful story of a visit to 
with hofstadter early on in the book why we start off by setting the stage with that kind of 
mm right so maybe five years ago some of the ai people at google invited douglas hofstadter who is 
very wellknown writer about ai thinker researcher they invited him to come and meet with them to talk about 
and thinking about how to push forward an ai i was living on the west coast in portland and 
meeting was in mountain view so he asked me if i wanted to come and i absolutely did so 
all showed up there and got to the google building and found our way to the conference room and 
got up to speak and he basically started telling the google engineers about how terrified he was about ai 
much he hated ai how much he feared it how much he loathed it he was extremely passionate and 
were just sitting there mouths agape sc not what they expected to hear mm thinking what the heck is 
on and they quizzed him and he his fear is a little bit complicated not just that ai is 
take over sc right the traditional fear that we hear about mm yeah sc so he has a slightly 
more different fear mm yeah his fear was more that in some sense all the things that he found 
most profound and close to his heart about human intelligence that been thinking about it for decades and decades 
carefully and thoroughly he is worried that these things will in some sense be too easy to automate that 
with its what he thinks of as cheap tricks will succeed in creating a person with a profundity of 
of his heroes like chopin or gödel or sc or bach or whoever mm or bach or whoever right 
that that would just horrify him that the human soul if you will would be so easy to capture 
a cheap machine sc and is the horror in the cheapness if it were hard to do but we 
still do it in a computer would he be less horrified mm i think sc or is that just 
ourselves into thinking that like that mm right so hard to speak for him but my impression is that 
harder it is the better sc right mm sc so we want to learn that human creativity is somehow 
hard and profound mm right so he extensively talks about chopin one of his favorite composers and feels one 
the most profound sort of humans who ever lived and he fears that something like music could be created 
an ai program that just used a lot of superficial heuristics and cheap tricks as he calls them sc 
random numbers match some patterns mm yeah sc and then you have the genius of chopin mm right and 
would just be the worst thing in the world to him sc well and there was an example right 
was a computer program that tried to mimic chopin mm right and did a too good a job so 
is a program written by a composer named david cope who is also a programmer who wrote this program 
experiments in musical intelligence or emi emi used a lot of ai techniques and sort of very relatively simple 
pattern matching techniques and it did a pretty good job of with lots of data to go on of 
in the style of various composers including chopin and doug even tried to play one of these pieces for 
group at the eastman school of music a bunch of music theorists and composers themselves and they were fooled 
know which piece was by the real chopin and which piece was by emi so that just terrified him 
then the google engineers at this meeting were very certain that human level ai of the kind that doug 
fearing was imminent in some way was at least we were getting much closer to it with deep learning 
all the datadriven approaches of today and i was just very confused about this whole meeting that doug hofstadter 
so terrified the google engineers were so glib about the idea that we were gonna have human level intelligence 
i find to be very unlikely mm so i decided this would be it kind of gave me the 
of trying to learn a lot more about ai i work in my own narrow area but to learn 
about it as a whole and to say like is the state of we read stuff in the media 
just seems to me to be outrageous hype and some people think that very close to human level ai 
people think extremely far away what is the real state of ai so kind of the topic of my 
i was sparked by this meeting at google and by terror which surprised me sc and you share the 
or the optimism really mm right i share either and the book is kind of an exposition of what 
found where ai is today sort of then we have to ask what do we mean by humanlevel intelligence 
absolutely yeah mm and illdefined we understand intelligence we understand the brain we understand the mind if they are 
separate things and i think that ai is now at a state where people just agree a huge amount 
disagreement in the field sc well noticed this and there are people who are not professional ai researchers but 
famous public intellectuals stephen hawking elon musk bill gates people like that who warn against ai coming in the 
and being super intelligent and taking over and they get a lot of pushback from people who spend their 
doing ai but i find the pushback especially convincing because you can be so immersed in the daytoday difficulties 
you miss the possibility of something big going on so what trying to sort of step back and ask 
in the book mm exactly yeah and interesting about ai which might be different from your field i know 
everybody has an opinion about it everybody has a about it everybody thinks they know what intelligence is and 
hard it is for ai to get it and we get a lot of people who are not in 
field who have never worked in this area opining chuckle with great confidence about the future which is very 
mm and i think most as you say most people who are kind of on the ground working daytoday 
with a lot of these more overthetop predictions but if you take someone like ray kurzweil okay who is 
of promotes the idea of the singularity where he thinks that ai is going to become at the level 
humans within the next ten years and then a billion times more intelligent than humans within twenty years or 
years or something he would say if sitting on an exponential and his idea that progress in ai is 
an exponential curve sitting in the middle of an exponential curve it look exponential to you but about to 
very very sc the nice thing about sitting on curves that look exponential is some of them might not 
exponential laughter mm exactly but he has books full of experiential curves that try and make the argument that 
are on an exponential curve and to people out in the world it kind of feels that way there 
so much progress been reported recently it does feel like been exponential progress and my book tries to look 
that progress and ask sort of how much progress has actually been made sc yeah and i think that 
just parenthetically i think there are almost no exponential curves in nature really exponential the expansion of the universe 
one of them but most curves look exponential at the beginning and then flatten off or decay away so 
think that the curve fitting tells us very much therefore dig into some of the details this is what 
book does really really well open up the hood a little bit and think about artificial intelligence you mentioned 
of the advances that people have seen recently we can talk to siri and so forth it almost is 
talking to a human and this is based on this is almost all based on deep learning and neural 
is that a fair thing to say mm yes absolutely so deep learning and deep neural networks which the 
of which is just neural networks with more than two layers laughter sc oh okay i was wondering what 
definition was because i had seem them used interchangeably but good mm yeah so typically a neural network has 
has inputs and then it has called a hidden layer or an internal layer and then it has an 
layer those are sort of the traditional neural networks from say the that people use so a deep network 
a network with more than two hidden layers sc very deep okay mm and the word deep should not 
confused with the words deep sort of in like sc profound chuckle mm thinking or profound or you know 
a very good word because it sc deep mm very deep to people but in any case deep neural 
with a lot of data big data to train them and very fast parallel computers to run them have 
a huge amount of success in some pretty narrow areas some of those areas are very useful so like 
siri speech recognition being able to transcribe our speech been an amazingly successful sort of breakthrough due to deep 
object recognition to some extent has also become really successful and useful or face recognition other kinds of computer 
have become very successful to deep learning but more and more people are beginning to see some of the 
so if you yeah sc well get to the limitations but actually i but first build our way mm 
sc what i should have said is me about the mm oh the perceptron sc this is a wonderful 
thing you talked about that led us eventually to neural networks mm right so the perceptron was maybe the 
of deep neural networks that was created in the by a psychologist named frank rosenblatt he was looking at 
models of neurons people have been making computer models of neurons ever since computers existed that was one of 
first things that people started to do and in that view a neuron was a very simple device that 
input if the input reaches some threshold internal threshold then the neuron fires sc so we knew enough about 
in the for him to be inspired by this to do a computer version mm exactly exactly they were 
threshold devices so to say and he found that if you create perceptrons if he created them in hardware 
you could see these in pictures of the perceptrons laughter that he created with these spaghetti masses of wires 
out and these giant main framelooking computers sc i forget would transistors have been around by then or was 
vacuum tubes in his perceptron mm oh a good question i sc it was about that time i think 
mm probably vacuum tubes but they were large laughter and they could do some things and the thing about 
that was so remarkable was that they could learn they were one of the first systems based on the 
that could do some learning and he developed an algorithm that allowed perceptrons to learn from examples sc but 
was really like one neuron was learning it was not there were not huge networks mm yeah essentially or 
network of some simple network of a group of neurons sc okay so there were some simple networks yeah 
yeah there were some simple networks and they would learn and they could learn to do things like recognize 
characters to some extent not very well not as well as humans but rosenblatt made a lot of promises 
people in ai tend to do it helps to get funding and he made a lot of promises for 
perceptrons would be able to do in the future and all of that was stomped upon by marvin minsky 
seymour papert in around one thousand nine hundred seventy when they did a mathematical study of perceptrons and showed 
perceptrons were actually very limited in principle in what they could learn even if you have as much data 
you want to train them they still cannot learn a lot of things that we want them to do 
and the idea just to make this absolutely clear to the audience the idea was that just like a 
neuron there are inputs from the equivalent of dendrites right mm right sc so some wires going into the 
that say pixel is lit up this much this lit up that and then some black magic happens inside 
perceptron and it just says yes or no the output mm right sc like this is what looking for 
no this is mm right sc and what was the difference then between that and a neural network mm 
two things one is that perceptrons had no hidden layer no hidden layer the word hidden is a little 
here but this is what people call it in the field it just means that a layer of neurons 
the inputs that where information propagates from the inputs to the hidden layer to the outputs and the hidden 
is able to somehow create some kind of internal representation of the input so if a hand written character 
showing it the input is just say all the pixels either black or white well the hidden layer might 
able to learn to represent something like if you want to say represent the handwritten digits like one zero 
nine you might be able to represent the idea of a loop or a circle so intermediate representation sc 
not just going on in one pixel some global holistic property mm right and this seems to be what 
brain does in some sc absolutely yeah mm rough sense in the visual cortex so perceptrons did not have 
hidden layer they just had an input layer and an output layer and the problem was that because they 
to create those internal representations they able to learn more complex functions more complex concepts and the problem with 
hidden layer is that there was no learning algorithm that could go from the outputs and look at the 
made at the outputs and apportion credit for or blame for those errors back through a hidden layer to 
input sc through the different neurons yeah mm yeah so it made the whole system much more complicated and 
had an algorithm for learning sc so the perceptron did have an algorithm for learning so it would basically 
with random inputs and outputs and then depending on how well it mm well random weights sc random weights 
what i mean yeah mm random weights so each of the inputs had a weight sc right mm right 
so the perception carries in different amounts about coming in from each of its mm right sc and then 
were set randomly and then it would make a guess what the answer is and if it guessed right 
sort that was happy and if it guessed wrong it would adjust itself a little bit mm it would 
its weights right and what the brain does neurons have inputs the connections between neurons their weight can be 
their strength could be changed in the synapse and what learning seems to consist of in the brain sc 
for the deep neural networks or just neural networks in general the trick was to teach the whole network 
to learn in that sense mm right and not until the later part of the people come up with 
algorithm called back propagation which was able to train neural networks that had these intermediate or hidden layers sc 
it was minsky and papert mm they come up with that algorithm sc no no sorry those are the 
who were skeptical about perceptrons and neural networks mm right and they when i went in and read some 
the historical literature i found that minsky and papert were very concerned about neural networks being sort of a 
for their approach to ai which is very different sc okay what was theirs mm theirs was called people 
it symbolic ai and it was much more based on humans programming and rules for behavior and instead of 
and also having the rules be interpretable to humans sc and the back propagation made neural networks much more 
than minsky and papert said they could be mm yeah minsky and papert actually doubted that there could be 
learning algorithm but they were very quickly shown to be wrong and they apologized later laughter sc okay good 
is that basically the kind of learning algorithms we still use mm absolutely sc yeah mm almost completely we 
use back sc okay mm which is based on calculus laughter sc good calculus always a good thing yeah 
and very elegant and it seems to work sc except see gone away a little bit from being inspired 
the brain because the sorry what do we call the individual nodes in the neural network neurons mm some 
call them neurons some people call them units sc units i wanna call neurons i care i care what 
neuroscientists say mm yeah yeah neuroscientists hate it when you call them neurons sc they can adjust for the 
of this podcast the individual we have a learning algorithm that propagates information backwards and forwards through the network 
in the real human brain different modules a visual cortex the amygdala and whatever and my impression correct me 
wrong is that neural networks start off pretty undifferentiated pretty homogeneous just like a whole bunch of neurons and 
train themselves and then wherever they go is wherever they end up mm right a lot of differences between 
networks in the brain most of the most successful neural networks people use today are very loosely based on 
way the visual system works at least as of the laughter and called convolutional neural networks so i know 
people in your audience probably have heard of these sc so what does that mean what does convolutional mean 
this context mm so the idea here is that if you look in the visual system each neuron say 
lowest layers of the visual system the visual system is organized into layers the lowest layer each neuron very 
this is a very rough approximation has input from say the retina and looking out of the visual scene 
sensitive to a particular part of the visual scene a small part sc yeah mm and what it does 
very roughly equivalent to a mathematical operation called the convolution where it essentially multiplies its weights of its inputs 
the input values in this small area and sums them up sc so sensitive to some things and not 
other things mm which makes it sensitive to some things and in particular the neuroscientists hubel and wiesel who 
this sort of structure found that these neurons were sensitive to edges which makes sense for vision like to 
where the edges are sc i think so yes mm and each neuron is sensitive to particular kinds of 
like some are sensitive to vertical edges some are sensitive to horizontal edges some are sensitive to edges in 
so what a convolutional network does is it has this idea of each neuron having a small sort of 
field that is it pays attention to a small part of the visual field and it does these convolutions 
are essentially detectors for things like edges nobody actually programs them to specifically detect edges but if you actually 
them on lots and lots of images the lowest layer in a convolutional network will develop edge detectors just 
the brain sc presumably because a useful thing for computers to be able to recognize just like us mm 
then a claim that they develop similar kinds of representations and even higher layers to seen in the brain 
a little bit harder to test and sc well this is a big part of the problem or at 
again in my mediumlevel understanding we can get deep learning neural networks that are very good at certain tasks 
then if you ask why they are good at it what is going on inside those hidden layers kind 
hard to suss out mm exactly neural networks are pretty complex systems they have especially the ones that are 
in real world tasks today have millions of weights in them some even billions of weights and really difficult 
figure out what doing not like the marvin minsky approach to ai where a human is building in the 
i say if driving in your car and you see a red stop light ahead stop instead that is 
included in these billions of weights in a way very hard to visualize or pinpoint sc and it makes 
this is jumping ahead maybe a little bit but it makes them easy to fool right a training set 
give them certain information if just trying to recognize digits or recognize stop signs or whatever and then it 
a certain thing and then if you know that good because of the certain training set you can give 
an image that we human beings would instantly recognize and they would get wrong mm yeah they are very 
to fool for sure and it seems that a lot of machine learning systems not just neural networks are 
to fool big data somehow causes networks to focus on certain features that make it fairly easy to fool 
if you know what doing sc i read recently that image recognition algorithms are much more sensitive to textures 
human beings are rather than edges so if you showed a picture of a cat with the skin texture 
an elephant it will certainly say an elephant mm right so something that i think a lot of people 
obvious when looking at these networks they perform really well think of a face recognition neural network you train 
on millions of faces and now able to recognize faces certain people we humans look at it and say 
at recognizing but you actually know for sure that what recognizing it may be that some aspect of the 
some as you say texture or something in the data that very sensitive to that happens to be statistically 
with certain faces and so not necessarily learning to recognize the way we recognize the way humans recognize it 
be something completely different that may not generalize well sc yeah when the context changes that correlation might completely 
away mm and something that people have found with these neural networks is that not only are we able 
fool them but even if not trying to fool them certain small changes in the data that given that 
different in certain ways from the input will cause them to fail one recent experiment they trained a neural 
to recognize fire engines fire trucks right then they photoshopped images of fire trucks in weird positions in the 
upside down sideways in the sky and the sc had no idea mm completely misclassified it even though a 
would be sc yeah mm would recognize them so then when we say trained them to recognize fire trucks 
totally clear what actually trained them to recognize a little bit of a difficulty in neural nets sc well 
this is one of the reasons why some of the most impressive successes of ai programs have been in 
welldefined finite situations like games right like chess and go and so forth mm yes sc clear what the 
are no equivalent of an upside down fire truck in a chess game mm right sc and then but 
a reflection of the fact that the way the network is doing it is a very different way of 
than human beings are doing it should we be very happy very impressed or a little bit skeptical about 
successes of these computers at winning in go or chess mm i would say both a little of both 
impressive chess and go are really hard games people train for years to be good at these games and 
machines have surpassed them in all of these games which is really impressive go has been a longterm grand 
for ai on the other hand not totally clear that these same techniques that make them so good at 
and chess are going to carry over to any real world if you sc yeah mm ability and we 
seen much of that sc well i guess my contrary take is never been impressed with the fact that 
and go are regimes in which computers can beat us i would think that those are the most obvious 
where computers should be able to beat us the impressive thing to me is that human beings with our 
limited number crunching capabilities in our head are pretty good at chess and go mm right sc if computers 
good at soccer and baseball be much more impressed mm well yeah but i think that not everyone saw 
that way a lot of people saw go and chess as the pinnacle of intellectual power if you would 
and a lot of very smart people literally said that back in the and that if a computer could 
chess at the level of a grand master ai would be solved chuckle sc just very backward i mean 
beings are really bad at taking cube roots of numbers and computers are really good at that and chess 
go seem to me kind of like that impressed by them in our fellow human beings because they seem 
things that computers should be good at my view i know maybe on the wrong track there mm no 
think very insightful and interesting but in some ways looking backwards already seen them sc i know too late 
should have written a skeptical book about this years ago mm so people in ai sometimes complain about this 
that once a computer can do it we no longer say it requires intelligence sc fair we are moving 
the real world right i did see some robots playing soccer but things like selfdriving cars and automated driving 
much more something pushing on very hard what is your view of how that field is advancing mm so 
cars is prototypical field for ai one where people think gonna be very easy that of the way there 
they think gonna be solved very soon and yet every time they try to deploy it in the real 
the real world bites back chuckle mm the real world is different from simulation different from all experimental techniques 
cars turned out to be a lot harder than people thought just like a lot of things in ai 
the reason seems to be that there are so many different possible things that can happen and i think 
is true in most of life not just driving but most of the time driving along in say on 
highway and cars in front of you cars in the back of you and nothing much is happening but 
something unexpected happens like a fire engine turns on its siren and starts coming by or there is a 
weed in the road i spent a lot of time in new mexico sc yeah laughter and even though 
events are unlikely crucially important that we get them right right mm yes and one of the problems with 
cars been told nowadays is that they perceive obstacles all the time even when no obstacle or human consider 
thing an obstacle and so they put on the breaks quite a bit sc you had the example of 
a snow man on the side of the road mm right so they know what to do say a 
man on the side of the road so unlikely but could happen a computer has no way of knowing 
that thing is not alive and is not gonna cross the street it has no way of knowing if 
dog is on a leash or not gonna run out in front of traffic it a lot of things 
humans can tell very clearly by their behavior what gonna do next but selfdriving cars have a hard time 
sc and it seemed to me that this is at the heart of it not just that these arenas 
more complex than chess or go but that a place where we humans have a competitive advantage we have 
picture of the world built into us that helps us answer some of these questions whereas if just a 
neural network trained on a bunch of test cases you have that common sense that we humans come equipped 
mm right so common sense is a really important idea in ai been talked about for years and kind 
an umbrella term for all the stuff that humans can do without even thinking about it sc yeah if 
put the glass on the table it will not fall through mm yeah all the stuff we know that 
either were born knowing or learned very early in life and we have models of the world that give 
common sense that we just have a vast amount of knowledge nobody knows how to model that in computers 
lot of effort being put into it sc well you mentioned this thing never heard of called cyc which 
short for encyclopedia in this case and i guess that was douglas mm lenat lenat sc lenat mm yes 
and an attempt to codify a decadeslong attempt to codify what human beings take for granted mm exactly sc 
just a list of propositions or how is it encoded mm encoded in a list of propositions in a 
language and able to make deductions and deduce new facts sc so what kinds of facts count as common 
to this program mm every every human has a mother every human has a father a human cannot be 
more than one place at a time if you leave a coffee cup full of coffee out in a 
room it will cool down everything you imagine listing all of the things that you know sc and how 
has this been going on the attempt to make all these mm maybe thirty years sc and there there 
guy said that they may be of the way mm yeah sc do you believe that figure whatsoever of 
way to understand common sense laughter mm no i have no idea how he got that that seems crazy 
mean no way you could possibly list all of the individual propositions of human knowledge sc and does it 
seem like the right thing to do maybe learned from certain artificial intelligence and just like alphago which is 
the best go player it they figured out that it was better just let a train itself rather than 
learn from human beings but maybe some advantage in this wider context of teaching it something about the world 
than just a list of true things like something about physics and something about the folk reality that we 
in mm right so humans have this knowledge that people call intuitive physics sc yeah mm i know that 
i drop a ball going to fall and i know if made of a certain material gonna bounce if 
of other material not gonna bounce i have all these things that i just know because either innate or 
learned them when i was a baby and a lot of theories about developmental psychology about how human babies 
things and animals and so on we also have some things that are even more basic than that we 
this idea of cause and effect that certain things can cause other things neural networks necessarily have that sc 
not at all mm they have no notion of causality but that seems to be really important we have 
notion there are objects in the world the world is divided into objects sc they have some permanence although 
not complete mm they have some permanence and that they have certain properties and neural network know that and 
clear that it could learn that sc so if some object disappears behind a barrier it would eventually reappear 
a human being would think but the neural network might have no idea mm but even the notion of 
object itself sc yeah mm so one example was in addition to alphago the google deep mind group did 
work on atari video games and there was one game in which you take a joy stick and you 
a paddle to hit a ball this is all in software hit a ball to knock out bricks called 
sc breakout i know it well mm breakout yeah a fun atari game so they they used reinforcement learning 
like in alphago to teach the machine how to play breakout it only learned from pixels on the screen 
have any notion built into it sc so it think i have a paddle a ball a wall it 
no but it learned to play at a superhuman level but then another group did an experiment where they 
the paddle and they moved it up two pixels now the program could not play the game at all 
it abstracted the notion of a paddle as an object just a bunch of pixels it was as if 
would see the world and not see objects sc but the lesson there is there a way that we 
there a more efficient way of teaching the computer how to think about the world to give it some 
sense mm we may have to build some things in it may be that things are built into our 
by evolution sc sure that would be the least surprising thing in the world really right mm yeah i 
but a thing a debate in cognitive science if you will or ai for one hundred years about innateness 
what we learned versus built in and no one really knows for sure but a lot of evidence that 
are subinnate concepts that we are born with just given to us and they bootstrap our learning we do 
without them and so we may have to build some things into our ai programs so this is just 
to deep learning people a lot of deep learning people because deep learning another machine learning was put in 
to the oldfashioned way of doing ai where everything was built in and that turned out to be very 
and not very adaptive so people want to just focus on learning everything from data sc i know that 
pearl the doyenne of causality gives ai researchers a hard time exactly for this reason that cause and effect 
are just not things that they program into the computers mm and not clear you can learn even the 
of cause and effect from a bunch of data sc well is there some intermediate where we teach the 
physics or intuition but we do teach them some higher level concepts we teach them metaphysics chuckle mm yes 
right sc we teach them objects and providence and cause and effect and they learn they could learn how 
dimensions there are in space maybe just by experiencing things mm right so there are some people who are 
on that kind of thing on building in some privatives and then having the systems say in virtual reality 
try and learn sc oh good how is that going chuckle mm some good demonstrations but not very general 
think a very hard thing to do and the virtual reality we have today is maybe i mean again 
reality can be very complicated too and so hard to learn sc do you think this is a promising 
forward to try to teach some of these fundamental issues no what should i say fundamental metaphysics mm yeah 
think metaphysics is a perfect word for that sc yeah it really is okay good mm yeah i think 
is and in fact darpa the defense advanced research projects agency which funds a lot of ai has a 
push on what they call foundations of common sense funding a lot of work on this sc ah really 
and their goal is to basically have the groups simulate a baby from zero to eighteen months they have 
the developmental milestones that gleaned from the psychology literature and they want the artificial baby to essentially go through 
developmental milestones it have to be an actual robot baby but some kind of simulation and gonna be tested 
the same kind of psychological experiments that people use on babies so their idea and gonna learn from videos 
virtual reality see chuckle sc see how that happens yes right but i wanna completely lose track of this 
you said about how this approach is getting pushback from the deep learning experts mm right so many people 
deep learning who feel that building things in is cheating sc is it just cheating or is it ineffective 
i think they feel that in some sense both just as it was back in the good oldfashioned ai 
that it makes the system unflexible that we know what to build in just we can build in rules 
those rules will be unflexible because we foresee every possibility and that things need to be learned by the 
sc are those the same people i guess not very sympathetic to this i have to say i do 
the common sense is secretly really really useful and training on huge data sets necessarily get you there i 
appeared on a podcast with lex fridman who is an artificial intelligence guy so i went as far as 
say maybe programs that try to recognize the number three the digits of the numerical system would be aided 
they understood the concept of three and he thought that was terrible he thought it was a very bad 
mm oh an interesting idea so there is a group who is working on this kind of thing thinking 
for instance joshua tenenbaum at mit and his group and some of his former students are looking at that 
of thing recognizing characters and by understanding the characters more deeply than just the visual sort of presentation of 
characters and one of the things they wanna be able to do is to understand how a character was 
in terms of the actual pen strokes and to be able to reproduce that and to learn from that 
as to be able to generate the thing as opposed to just recognize it sc yeah my worry is 
if we teach computers this way they will no longer be able to take cube roots or play chess 
well or anything like that become too humanlike mm well that may be an inevitable evolution so in doug 
gödel escher this was written in the so a long time ago he actually has a set of ten 
and speculations on ai and one of his questions was will a smart computer be able to add fast 
is exactly that question sc yeah and what was his answer mm his answer was no because exactly what 
said mm our numbers to us the concept of three is a fullfledged concept that has a lot of 
sc yeah a lot of baggage chuckle mm exactly a lot of baggage and when we think about three 
just have some bit string representation we have all kinds of associations with it which maybe makes it hard 
us to add things sc i did read gödel escher bach when i was a kid so maybe this 
why i have these infarctions mm yeah no a great question and i think to me i always found 
one of the most surprising things i read in that book but thinking about it i think it makes 
lot of sense that maybe there are tradeoffs that we have humanlike concepts and also get rid of all 
our human slowness and the rationality and cognitive biases and all of what have you sc i do think 
as a practical matter there be a problem giving a computer a subroutine that can add and subtract and 
and divide mm we can give it a calculate sc yeah exactly mm just like but it feel that 
calculator is part of it sc right right mm not part of itself sc well this is getting us 
thing to be generally intelligent in the sense i think general intelligent is not the right phrase one thing 
an ai to be able to work in the real world like selfdriving cars another thing for it to 
humanlike in its intelligence are you someone who thinks that we are going to get there any day or 
google people in your introduction seemed very optimistic mm yes and a lot of people are very optimistic not 
optimistic because i think human level intelligence is a lot more complex than people realize a lot of unconscious 
experience a lot of our intelligence consciously and this is a problem that i think bleeds into view of 
they think certain things are very easy and and one of the things i quoted marvin minsky on is 
things are hard sc a great quote yeah mm that things that we think are very easy like looking 
in the world and describing what we see or recognizing your mother or all these things turn to be 
out to be very difficult in general for computers whereas the things we feel are very hard like playing 
they turn out to be able to do well sc well we do have chat bots we do have 
things like that we can mimic at least the rudimentaries of human speech interaction mm right sc the rudiments 
the rudiments so a big question is is that on the right path to actually fullblown humanlike conversation sc 
quickest way to do something well might not be the right way to eventually doing it well mm right 
hubert dreyfus a philosopher who was a wellknown ai skeptic had this analogy that you could climb the tallest 
around and say you were closer to the moon but really best way to get to the moon is 
get down from that tree and get on a rocket ship sc yeah that seems very a good analogy 
many problems in ai one issue is embodiment i think a few other times in the podcast when talked 
to computer scientists but to neuroscientists the point is made that just human thinking is so enormously affected by 
fact that the brain is in the nervous system is in the body is in the environment and to 
a lot of ai is still on a computer screen not in a robot mm right i think ai 
are secretly cartesian in their dualism they admit it but they believe they believe that intelligence is all in 
brain there are a lot of people who are doing what they call embodied ai mostly having to do 
the robots where actual sort of intelligence is spread you have to have the right kind of body i 
to agree with the embodied people that we have humanlike intelligence without a body i go so far as 
say we have intelligence i really know what that term could possibly entail but humanlike intelligence is very much 
in our bodies how we think about concepts we think in terms of metaphors having to do with the 
and physical space and time we understand abstract concepts and analogies very physically so i think just going to 
more and more as we try and push ai further and further sc well one thing that ai is 
at maybe not ai but computers are good at these days are deep fakes making images or even videos 
look like another person is actually saying them so sure people have imagined this but it happen before too 
that we wed a chatbot or something like siri to a deep fake and can actually mimic not only 
video of someone saying something but an interactive conversation with an actual person and you know think facetiming somebody 
just face timing an ai mm that definitely could happen but i think right now siri and its cousins 
good enough to fake it sc not right now mm so a really good question sc always the question 
how fast these improvements are mm how will more data make them better turns out that language understanding is 
more difficult than vision which is interesting maybe because at least the kind of vision in terms of object 
and so on that language understanding involves common sense it requires a huge amount of knowledge and ai is 
less advanced in language than it is in vision i would say sc this is why the turing test 
hard to pass right mm well it depends on the details so there have been chatbots that have the 
with certain kinds of judges really hard a lot of people are very willing to anthropomorphise computers and to 
that actually humanlike sc i think what it is whenever i see these reports of a computer winning the 
test passing the turing test and then you see some of the actual conversations just shaking my head and 
like in the world can you think that was a mm yeah i mean if you had to interact 
siri for twenty minutes talking to it it would never fool you into thinking it was a person sc 
do wonder whether or not we also though overestimate how complicated people are is it conceivable that many people 
have a relatively limited set of things they would say or do in any certain set of circumstances and 
might be mimicable in the foreseeable future mm it depends i guess it depends on the idea of limited 
limited but sure i think these chatbots can carry on a conversation a lot of times they have what 
might call cheap tricks they change the subject a lot they answer a question with a question they are 
to deal with when they sc sticky situation yeah mm yeah but sure one chatbot i know that learns 
just kind of repeats different things that heard from other people and it sounds kind of gamelike sc well 
thinking i like many people who are not experts in this field heavily influenced by science fiction depictions of 
things mm oh right sc and iain banks i know if ever read his science fiction books mm no 
but one of the concepts that he has is that people can be mimicked by computers at some percent 
you can basically download not a copy of your brain but a much more compact representation of what you 
usually say in typical circumstances and that can outlive you when you die right and so like the portraits 
harry potter where sort of a slim faint reflection of who you really were but something that people could 
interact with after gone that seem crazily unreachable to me mm yeah i think you would never actually be 
by it sc yeah you be food but you might be able to say some things learn some things 
amused at the very least mm yeah sure a lot of the things that i say every day are 
stereotypical whenever sending an email and i get the little suggestion from google mail about what about to say 
right chuckle sc pretty good yeah right right mm and which means that just very predictable and boring in 
lot of ways but i like to think that maybe that sc it matters mm it get actually matters 
lot sc but we already are in a world where holograms are doing world tours holograms of dead musicians 
on tour now and maybe given little bit of interactivity you could go a long way need to carry 
a conversation need to be able to write a song but it could respond to the audience or to 
musicians mm yeah i think that kind of thing could happen and i do think a lot of cheap 
can be very effective if you like sc right mm the question is is it cheap tricks all the 
down sc yeah mm and i hope not i think so but it could be i could be wrong 
well and the cheap it sort of leans both ways one of the things you talk about in your 
are the ethics the values that we are gonna attribute to ais the fact that an algorithm can be 
i think is one that some people have difficulty wrapping their minds around mm right algorithms learn from data 
if data is biased the algorithms are biased they just pick it up from the data we have algorithms 
face recognition algorithms that are much better on white males because been trained on data sets that are majority 
males trained on images that are uploaded to the internet and those tend to be wellknown people statistically who 
to be white males sc wellknown to us yeah to be wellknown mm and so people in ai have 
that these systems which people are using commercially are much more likely to make errors on say africanamericans there 
a really scary headline i saw that said selfdriving cars are much more likely to hit black pedestrians than 
pedestrians sc wow i did not see that one but when you say it yeah mm you would think 
yeah and a lot of the language systems have bias in their language that i saw one system question 
system that would answer questions about photos and if you show it a photo of somebody cooking in a 
much more likely to think that a woman than a man because see more women cooking sc yeah not 
women are intrinsically the ones who cook but if you train them to think that what gonna think mm 
these things can actually matter in real life because they if say these biases are kind of hidden under 
surface if used in the real world for these systems making decisions and the decisions are not very transparent 
being made the biases can kind of creep into them so a lot of concern about that sc and 
i know do you have a proposal for what to do about this or how we should be on 
guard mm oh wow i think we need to have developed some tests of systems bias we have people 
have had these implicit bias tests they give to other people and those are very revealing you could also 
such tests to give to ai systems sc i see so gonna start giving psychological tests to our computers 
yeah absolutely and we have to be more concerned about how we form data sets how representative they are 
so is the ethics not only what the ai does but of how we teach it mm exactly yeah 
then people who are trying to find ways to ai with limited success so far if you debias them 
much they start to make a lot of mistakes it hurts their performance sc right mm so an open 
area but important sc and do you have worries about superintelligent ai taking over the world mm the least 
my worries right now i have a lot of worries but the least of them a lot more worried 
say these deep fakes for example and just general fake media and worried about most like the issues of 
and privacy worried about the companies deploying ai systems that ready that are that have enough sort of quality 
sc people use ai for things like looking at resumes to pick job candidates right mm yeah sc or 
recommendations in the justice system mm yeah use ai for all kinds of things deciding if gonna get a 
your bank is using ai and deciding whether you qualify for food stamps more and more people are using 
to make decisions for us and it worries me but superintelligent ai seems to me to be very far 
sc what would it mean what exactly does superintelligence mean mm well a thing not even sure not even 
possible intelligence well defined one of those terms that we throw around all the time but it means so 
different things so intelligence is if you were talking about sort of animal intelligence very ecologically specific specific to 
niches animals including humans are adapted for certain kinds of things they do certain kinds of things well they 
certain kinds of things well because it helped them to survive and so when you talk about intelligence as 
this sort of monolithic thing oh gonna have super intelligent machines hard to know exactly what that means right 
something could be sort of better adapted maybe these machines could i was listening to one of your previous 
about how hard it is to understand the foundations of quantum mechanics maybe humans just have the right kind 
brain for that maybe you need a machine to do that to figure that out because they have a 
sort of ecological niche of understanding but would we call that a super intelligent machine i know sc yeah 
think this is a very good question i just last week was at a conference and heard max tegmark 
a report on stuff been doing with his students on ai physicists so theoretical physicists modeled by ais but 
really doing is very very fancy curve fitting they see a bunch of data points and they find the 
of physics that best fits those data points but asking what is the ontology of reality that would best 
quantum mechanics or something like that is a little bit harder mm so being very speculative because i think 
machine now has anything like the concepts that we have that would allow it to do science in any 
any meaningful way but if you have to ask is superintelligent ai even possible you have to start thinking 
what you mean by that and what we mean by intelligence and nobody really has a good handle on 
we mean sc well i think the impression i got from your book is ai has become very good 
very certain kinds of things the kinds of things that are naturally adapted to brute force and deep learning 
and a whole other set of kind of things that really are especially that humans are especially good at 
the commonsensical things maybe even the creative things the creative things actually less sure about than the commonsensical things 
are the lessons that this teaches us for how we should think about ai and how we should do 
on it mm okay a hard question i know if ai is intrinsically limited ai of course just one 
either sc sure mm i know if machines say are intrinsically limited that things that machines are good at 
things that good at i think of us as kind of fancy machines sc fair enough on board with 
there mm so not sure any intrinsic limitations but we could talk about the current approaches to ai this 
of datadriven approach probably certain things that good at and certain things that not good at seen that so 
me there could be two lessons for research one would be to try and understand better what kinds of 
that ai are good at and how to maybe make it more reliable or less vulnerable to fooling fooling 
more of trustworthy and so on but then the research into what ai is not good at to say 
what is it about us that current machines do and a lot of things that we can do very 
one of them is forming abstract concepts and something particularly interested in and so all of my research now 
on more on that side trying to say what these machines do and why not what are they lacking 
the reason investigating this is not to try and build smarter machines per se but more because i wanna 
what intelligence is and how maybe we do it or how it could be done more broadly so i 
of it as more of a scientific than engineering approach sc so give us a hint well how do 
form an abstract concept is this something that learned something about in the research or why is it hard 
why is it hard well okay interesting so an example think of the concept of sameness like two things 
the same an abstract concept right very abstract and something that deep neural networks do even in the simplest 
they recognize if you give them an example of something in which two things are the same versus two 
are different in general it do that task whereas we humans can very easily sc well i would say 
kind of if you ask people what they mean by sameness then gonna quickly get into a philosophical argument 
right right and a friend of mine actually wrote a book called the subtlety of sameness sc there you 
mm which is one of my favorite book titles ever and it is extremely subtle but we seem to 
very good at it and kind of the root of all analogy analogy is a really important part of 
and essentially seeing two situations as being two rather different situations as being intrinsically the same i look at 
particular political situation and i for instance think back in history we called the irancontra deal irangate that was 
an analogy with watergate right very different situations but somehow we saw them as the same kind of thing 
coverup a government coverup right so a very subtle kind of sameness and sort of underlying a lot of 
thinking that we do in terms of analogy sc well you have these wonderful examples and maybe from douglas 
but how human beings think by asking told that abc gets converted to abd then what should pqrs be 
right mm right so that that letter string analogies was a subject of my phd dissertation actually sc there 
go mm so i built a program that could make those kinds of analogies in what we claimed was 
humanlike way sc because no right way right mm yeah sc so give some of the possible answers to 
question what should pqrs be converted to if abc is converted to abd mm so abc is converted to 
what should pqrs go to well most people would say because they see abc changed to abd the rightmost 
replaced by its successor okay but you could also say replace the rightmost letter by a d or you 
say no or in pqrs so change anything sc if the rule was change every c to a d 
change every c to a d right or another answer could be abd change any string to abd you 
just go on and on and on but people people are very good at being sort of abstract in 
right way right being i guess defined to what people do sc well in a way yeah right a 
way that people would do mm in a way yeah yeah humans are very consistent right so the idea 
that whole project was not to solve letter string analogy problems but more to model analogy in general and 
say that this little micro world of letter strings captured some of our analogical abilities in general sc i 
this is a deep thing because there are an infinite number of rules that would always explain some situation 
is the duhemquine thesis in philosophy of science have you ever heard of that mm yeah sc any set 
data can be fit by an infinite number of theories and i had a math professor when i was 
undergraduate who hated the sat and gre series test when given a sequence of numbers and told to guess 
next one and he goes it could be anything i could invent any series of numbers that could be 
by anything but i think that the point my response to him was that they actually math questions they 
science questions they were looking for patterns in the world and not all patterns are created equal so in 
sense either because we humans or the world we live in with its actual rules acts in certain ways 
therefore it makes more sense to us that certain patterns are the right ones rather than the wrong ones 
it comes down to exactly this common sense right this way of dealing with the world that computers are 
very good at mm right sc and what did you learn by doing this phd thesis about analogies mm 
that they can be very subtle and what did i learn well we doug hofstadter and i developed an 
sort of a computer architecture for solving them that i think is more widely applicable so working on how 
this apply how can we apply these ideas and more broadly and i think that an area of research 
been looked at very much in ai the whole area of analogy and sort of common sense analogy sc 
you is there a movement in ai not just a few people but is there a growing feeling that 
learning is a kind of thing but we need more common sense we need more folk intuition more metaphysics 
our computers mm a lot of people say that not everybody sc okay mm yeah i was kind of 
recently the turing award which is the highest award in computer science was given to three deep learning people 
two of them geoffrey hinton and yann lecun both gave a speech at one of the big ai or 
conference yeah and hinton particularly was incredibly optimistic about deep learning whereas lecun was saying we need something sc 
there any chance that we could learn more about deep learning or make it better just by asking it 
thinking laughter are these demanding that when the algorithms reaches a certain conclusion it can explain itself mm some 
are trying to do that but hard since first of all to explain itself it has to have concepts 
has to have a concept of what it did it just say i multiplied these four billion weights by 
inputs and then got this explaining is kind of relative to the entity explaining to sc yeah mm and 
if it wants to explain itself to humans it has to have some kind of theory of mind of 
counts as an explanation to a human sc so i wanna be the deep learning proponent instead of skeptic 
the purposes of conversation maybe it could learn that through deep learning maybe if we insist of one of 
success criteria is it can explain itself then maybe it will be forced to come up with concepts and 
them together mm interesting sc easy for me to say mm yeah laughter mm yeah have to figure out 
kind of what language it would explain itself in i you know that sounds hard to me but i 
sc sure hard mm it is important this whole area called metacognition that people look at and the idea 
we think about our own thinking able to explain why we did things not always correctly you sometimes rationalize 
we did something or why we made a decision but metacognition is really important part of intelligence and something 
people in ai kind of some areas of ai have kind of looked at but nobody has any good 
about how to do it sc i mean maybe what doing is the equivalent of giving our deep learning 
multiplechoice tests when we really should be asking them to show their work mm yes laughter mm yes exactly 
alright just to close up then let her hair down and the end of the podcast and prognosticate about 
future a little bit i know that always very hard and i promise not to hold any bad prediction 
make against you fifty years from now just as one data point what do you think the landscape will 
like fifty years from now in terms of ai in terms of how generalpurpose it will be how much 
sense it will have how close it will come to being humanly intelligent mm fifty years from now wow 
you can change it to another time but i think fifty years is good because on the one hand 
be dead mm yeah will the world last that long sc on the other hand like you maybe you 
accurately guess ten years from now but no one can guess accurately fifty years from now so mm yeah 
well i can imagine that we would have much better chatbots that can the deep fake stuff would be 
good which is terrifying sc yeah so there recently this demonstration of a basically what a chatbot was basically 
that was a chatbot called up a restaurant and made reservations and it really fooled the person on the 
end had no idea he was talking to a robot mm right and a lot of conversation about should 
be required to identify itself as a robot and i think that kind be a lot of legal frameworks 
up over the next fifty years about ai and that kind of thing a little scared that we might 
in a complete surveillance state fifty years that everything will be known about everybody sc yeah mm and all 
that data will be used who knows what target ads to us sc it seems basically inevitable to me 
have you seen the movie minority report sc i have yes mm yeah like a real dystopia where it 
you it does an iris scan and figures out who you are and then constantly bombarding ads at you 
completely towards you or individually horrifying but if you look at current trends almost where heading sc well yeah 
i think even much worse than targeting ads like what if we become really good at deciding who will 
on a loan and therefore we give them a loan even if they defaulted yet right mm right sc 
some mm and the whole minority report plot where we decide if gonna commit a crime sc yeah trying 
do that perfectly legal for a mortgage company to try to decide the probability you will default on your 
right mm yes sc but if we become infinitely good at it or at least we think infinitely good 
it then a different thing than what is the difference between that and saying probably commit a burglary mm 
i think also because of coevolution with these technologies that a lot of people will be working on how 
fool them how to thwart them how to avoid this kind of thing and there will be a big 
of coevolutionary arms race between the technology and the people who wanna be controlled by it sc presumably some 
things maybe when i call to change my flight reservations get a much more useful computer interface than i 
right now chuckle mm but audiovisual be solved like being able to get your slides to show up on 
computer sc yeah no no no true but i now do you think that will you said audiovisual and 
brain went to the wrong place will it be video calls rather than audio calls this is mm you 
we already have that sc we have it but people use it that much right mm no yeah probably 
will use it more sc yeah weirdly we use text messaging now more than phone calls so that gone 
in the amount of information sent mm yeah sc the kids today they like to text on purpose mm 
yeah we possible that people wanna interact at all laughter sc well again trying to look at the sunny 
of this thing we can imagine that if we do have the equivalent of deep fake chatbots they will 
useful companions for people for lonely people for elderly people or whatever a sort of robots who are already 
this but i think become better at that mm yeah right right and the effects of that are completely 
sc yeah mm i find that a little scary sc are you mostly scary or are you mostly excited 
the robot ai future mm it varies very excited i tend to get more excited when things work sc 
scientist of course sure mm yeah i wanna understand why they work and so i know if that makes 
kind of a pessimist i think a lot of sort of this is a whole different discussion but sort 
the academic research in ai is badly broken in many ways in that people tend to focus on particular 
data sets and see who gets the best performance on them and the stateofart in the scientific approach to 
really progressing very much i think sc sorry the relationship between those two facts mm oh well suppose that 
want to get your paper published in the top conferences in ai the best thing to do is to 
a benchmark dataset that a lot of other people have worked on and to do better than anybody else 
to explain anything or to understand why your system did better sc so saying that becoming like the deep 
networks mm we are being optimized according to these benchmarks right we get rewards in the form of grant 
and sc yeah mm recognition sc it always been like that is really not a new thing mm i 
i know maybe a little bit sc always optimized for our lock of reward system i think mm yeah 
not always the best for you know progression of science sc true hard like science does progress pretty well 
the fact that there are so many obvious flaws in the system that hard to know where to start 
them mm yeah chuckle sc but i like your philosophy of being excited and happy when things break and 
wrong i will count that as an optimistic stopping place because sure things will continue to break and go 
mm absolutely sc alright melanie mitchell thanks so much for being on the podcast mm thanks for having me 
stuart and mellanie book to read in the same week supplemented with a mindscape i guess a glitch in 
aside from maybe anything by jeff hawkins this is my favorite podcast episode on ai if anything have used 
anthropomorphic language when referencing computers eg instead of but just my pet peeve extra points for releasing this one 
before hubert birthday really insightful podcast i like your left field thinking sean too especially on not thinking winning 
competitions is remarkable the whole conversation seemed like it deserved to mention the chinese room thought experiment neural nets 
machine learning will never be melanie mitchell mischaracterized convolutional neural networks what she described if i understood her correctly 
simply the feedforward calculation in which the inputs are multiplied by the weights in the early layers of a 
neural network each node receives input from only a small part of the previous layer which may be the 
layer analogous to a receptive field and these receptive fields tile the input space the essential feature of a 
network is that the same weights are used for all of these nodes in learning the weight changes that 
reduce the error are calculated for each of these nodes then those changes are averaged across all of the 
and the average indicated change is made in the single shared set of weights that all the nodes use 
is interesting that even though we understand the subjective experience of human beings we somehow believe that it may 
possible to create machines that have subjective experiences scientists who are working hard to advance machine computation are doing 
work and are to be congratulated but until far more is learned about human intelligence it seems unlikely that 
machines will be created the experience of helen keller is instructive she lost both her sight and her hearing 
the age of nineteen months however as a child due to the dedicated efforts of a teacher she learned 
the water she felt on the palm of her hand was represented by the word then scribbled on her 
that single aha moment led to a lifetime of subjective experience and intellectual achievement would it be possible to 
a machine that could do that really insightful discussion thanks sean point about cause and effect is fundamental the 
difference between ai and living brains is that living brains are embodied and they learn cause and effect via 
from their own interactions with the environment do something suggestion that some key concepts like object permanence might be 
strike me as at best half true and it worst false more likely object permanence is learned inductively via 
on interactions with objects though it is likely that the propensity to learn specific concepts is an emergent property 
neural wiring the conclusion is that any human level ai would need to be embodied and have so many 
loops of interactions with environment that it would to all intents and purposes be a living thing very good 
read melanie books and she is very clear and provides very good insight her complexity book allowed me to 
understand complexity and some of the applications she provides a good definition on deep learning deep neural networks this 
one of my favorite ai podcasts being in a trained physicist and working in the field of machine learning 
really enjoyed this episode especially besides all the other great episodes your podcast sean is trully one of the 
out there melanie and you shortly picked up on one of the upcoming fields of ai research learning causality 
think its really a great topic and i just wanted to note that there is actually some active research 
on this field one of the driving persons in this area is bernhard schölkopf of maxplancksociety from my view 
would be really great to hear an episode with the two of you maybe you think the same thanks 
much httpsmitpressmitedubookselementscausalinference httpswwwismpgdebs is interesting that even though we understand the subjective experience of human beings we somehow believe 
it may be possible to create machines that have subjective experiences scientists who are working hard to advance machine 
are doing important work and are to be interesting to me that people think there is something magical about 
that be achieved in silicon as soon as new company neurolink rolls out new chip we will be able 
record neural data at of bits per second from each person then train a neural network with enough data 
you may have a conscious machine already comments are closed sean carroll hosts conversations with the worlds most interesting 
science society philosophy culture arts and ideas 