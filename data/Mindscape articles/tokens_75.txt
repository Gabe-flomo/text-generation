
news that computers are exerting ever more influence over our lives and beginning to see the first glimmers of 
kind of artificial intelligence computer programs have become much better than humans at welldefined jobs like playing chess and 
and are increasingly called upon for messier tasks like driving cars once we leave the highly constrained sphere of 
games and enter the real world of human actions our artificial intelligences are going to have to make choices 
the best course of action in unclear circumstances they will have to learn to be ethical i talk to 
leben about what this might mean and what kind of ethics our computers should be taught a wideranging discussion 
computer science philosophy economics and game theory support mindscape on patreon or paypal derek leben received his phd in 
from johns hopkins university in he is currently an associate professor of philosophy at the university of pittsburgh at 
he is the author of ethics for robots how to design a moral algorithm web pagephilpapers profileuniversity web pageethics 
robotsethicsforrobots rawlsian algorithm for autonomous click to show episode transcript click above to close sean carroll hello everyone and 
to the mindscape podcast your host sean carroll and episode gonna see where the rubber hits the road in 
philosophy and i mean that quite literally all heard about selfdriving cars and you may have heard about the 
that selfdriving cars are going to have to solve the trolley problem this famous thought experiment in philosophy where 
can either continue to do something and several people will die or you can take an action to prevent 
current course of action and do something different and fewer people will die is it okay to intentionally kill 
smaller number of people to save a larger number of people hardly news that computers are exerting ever more 
over our lives and beginning to see the first glimmers of some kind of artificial intelligence computer programs have 
much better than humans at welldefined jobs like playing chess and go and are increasingly called upon for messier 
like driving cars once we leave the highly constrained sphere of artificial games and enter the real world of 
actions our artificial intelligences are going to have to make choices about the best course of action in unclear 
they will have to learn to be ethical i talk to derek leben about what this might mean and 
kind of ethics our computers should be taught a wideranging discussion involving computer science philosophy economics and game theory 
leben received his phd in philosopy from johns hopkins university in he is currently an associate professor of philosophy 
the university of pittsburgh at johnstown he is the author of ethics for robots how to design a moral 
click to show episode transcript sc you might not think that this is something you are going to need 
deal with but simply an illustration of the kinds of problems that all sorts of robots and artificial intelligences 
going to have to deal with going to need to make choices and in the extreme examples going to 
to make hard choices about how to cause the least harm as one example should a selfdriving car if 
are two bicyclists in the way and it judges that going to have to hit one of them should 
selfdriving target a bicyclist with a helmet rather than one without on the theory that wearing a helmet makes 
bicyclist more safe and therefore in some sense that person should get punished for wearing the helmet that seem 
sc these are moral intuitions that lead to really hard problems and we have to face up to them 
derek leben is a philosopher who has written a new book called for where he tackles exactly these questions 
just selfdriving cars but the general idea of what kind of moral decision processes should we program into our 
intelligences i think just a fascinating topic to think about because on the one hand book involves big ideas 
moral philosophy utilitarianism versus deontology john theory of justice things like that on the other hand very downtoearth questions 
game theory the prisoners dilemma nash equilibrium pareto optimality other sort of economic and rationalityoriented ideas need to come 
play here sc so to me a great example of how the abstract theorizing the philosophy suddenly becomes frighteningly 
to real world decisions personally i do not own or have plans to buy a selfdriving car in the 
future but i do think coming moreover artificial intelligences of all sorts are all around us and have an 
effect on our lives therefore we should be thinking about these issues and now is a good time as 
go music sc derek leben welcome to the mindscape podcast derek leben thanks so much for having me sc 
this is a topic ethics and morality for robots and artificial intelligence thinking about this we all know selfdriving 
are coming and gonna be apparently running into people on the streets right and left just deciding how many 
to hit can you just give us your short version of why necessary even to talk about ethics or 
for robots i mean do they even have ethics should they just do all we program them to do 
yeah so a great place to start as we are starting to develop more and more autonomous technologies in 
fields of transportation medicine warfare they are starting to make these decisions that are going to have impacts on 
health and safety and opportunity and for that reason we need to start thinking about which actions are permissible 
them to do and which actions are impermissible and i take this to be just an inescapable fact of 
machines that are making complicated decisions about human wellbeing that are going to be making these decisions without very 
human supervision right just gonna have to inevitably decide what kinds of rules we want these machines to be 
now not sure if we could use certain words to talk about these machines like responsible or not or 
worthy or not but sort of beside the point for me what most interested in is what are the 
that actually going to be using to program into these machines because definitely something that we need to do 
yeah you have used the word decisions as if the robots have the ability to make decisions is that 
important word or is it just a way of talking about the fact that the robots are gonna be 
something and we have to decide what we want for them to do dl interesting for me it really 
whether you call this a decision or an algorithm i know that maybe some people might think of decision 
something performed by an agent with free will who could have done otherwise or something like that but for 
not too important not going to get hung up in those kinds of issues i mean certainly some moral 
would like a kantian about ethics is going to say that only an agent with free will who understands 
he or she is doing is capable of being a moral agent who makes decisions at all but that 
already reveal something about my normative assumptions that making and something that i think see as we move forward 
every kind of choice you make in programming a machine is revealing something about your normative assumptions sort of 
to stay free of ethics to just say going to avoid ethics sc right well i agree with that 
much and also happy that it matter whether we attribute free will to the robots because i hate talking 
free will and yet i end up doing it all the time so glad that we can avoid doing 
for this one okay so get one thing out of the way which sure been hit with before what 
isaac asimov he explain to us how to make moral rules for robots he have three laws and we 
implement them dl yeah so asimov did propose these three rules which are really just one rule and two 
of subservient rules that say obey people and protect yourself but the first rule is really the one doing 
the work and it says cause or allow harm to other humans now on the face of it actually 
good and i think the reason why a sort of variety of moral theories and a variety of different 
of rule systems that constructed is that most of them do pretty well in normal circumstances a bit excuse 
gonna make the first of many analogies to physics here because i love to make analogies to physics and 
is my chance a bit like using classical mechanics in most normal circumstances where if not going very fast 
dealing with moderately sized objects this works really well dl however when you get to these very sort of 
situations you start to see differences between the moral theories and the problem with laws is that vague about 
situations like the violation of property rights the violation of dignity insulting people does that count as harming them 
trespassing on their property count as harming them does blowing smoke in direction count as harming them and also 
fails in these situations where every action either does or allows harm to others and these are called moral 
so in some scenarios you avoid either causing or allowing harm to others and law simply breaks down sc 
to be perfectly honest is the question that i asked you was because i feel i have to ask 
but i think being far too generous to laws i think just silly the idea that a robot cannot 
inaction allow a human being to come to harm is entirely impractical like human beings come to harm over 
world all the time every robot would instantly spring into action trying to prevent every human from stubbing its 
right dl yeah exactly and this is a point that philosophers like peter singer have made for decades is 
almost everything we do in the world has some kind of effect on people that we might not even 
aware of singer has gone to great lengths to show that the way that we eat the way that 
travel the way that we spend our money is actually having effects on other people we could be doing 
things with that money with that food perhaps driving around in our cars not thinking about the damage that 
to the environment and the future generations sc and i also like the idea that you need to be 
little bit more specific i think a lot of people do have this idea that the example i used 
my book the big picture was in bill and excellent adventure we heard the moral rule that you should 
be excellent to each other and i try to make the point fine but not quite good enough chuckle 
told us what excellence is and actually recently i got into a twitter conversation with ed solomon who was 
screenwriter for bill excellent adventure and he was very pleased that even if i was saying not a good 
that it made it into this kind of consideration and certainly when robots are on the scene we have 
be a little bit more specific a little bit more clear a little bit more quantitative maybe about what 
a morally correct action is that right dl right and i agree that what describing is a theory that 
are familiar with virtue ethics and it basically says whatever a noble person would now this kind of works 
you have a good exemplar of a noble person handy but then there are all sorts of problems like 
do you know that this person is actually a noble person whose behavior we should be trying to imitate 
not and if you are a noble person in one culture that might be a very very terrible person 
another culture and so that leads to this problem that you were just talking about which is where do 
look for these more precise more quantitative more formal approaches to designing moral algorithms and a lot of different 
one place we might look is in human judgments and try to model machine behavior after human behavior another 
we might look is to moral theories and try to actually take a historically important theory and implement it 
a machine sc and so yeah in your book ethics for robots a wonderful little book everyone can read 
and it really delves into a lot of fun things am i accurate in saying that you contrast three 
approaches here utilitarianism libertarianism and contractarianism dl right yeah i talk about some historically influential moral theories utilitarianism kantian 
contractarianism and you could also include some other ones you could talk about virtue ethics if interested in that 
not but like i said i think that virtue ethics is going to be specific enough to make it 
into this club sc be a virtuous selfdriving car is hard to actually implement in real life dl it 
unless doing something like train the machine in a sort of bottomup approach as they say to be like 
beings around us actually a few people like wendell wallach and colin allen who have proposed that this is 
good approach to designing moral machines however my objection to that is that probably going to incorporate all of 
terrible biases and limitations of the humans that using to model your behavior on so these machines are probably 
to wind up being terribly racist and sexist and not thinking very clearly about consequences versus rewards and so 
sc and also it just a little bit circular what it means to be moral is to be a 
person i think on your side here if right that you would say we need to be much more 
both for robots and for human beings about what it objectively means to be a moral person dl yes 
totally agree so it sounds like we are both on board with virtue ethics not necessarily being a good 
here maybe we can move on to what you were talking about which is these historically important moral theories 
in your book the big picture which i also am a fan of and i recommend you talk a 
about how these theories are constructed around our moral intuitions sc right dl and there might be different consistent 
consistent sets of rules that you could get from different kinds of moral intuitions and i actually think that 
completely correct i think that that is historically where these theories have emerged from and the question is not 
much well which intuitions should we rely on but what is i think the evolutionary function of the intuitions 
we are using in the first place so one answer to this question is a lot of these different 
consistent sets of rules that are all more or less intuitive in some kinds of circumstances but we have 
way of evaluating one versus the other if somebody wants to be a utilitarian and say wrong to for 
buy a cappuccino when you should be giving money to famine relief and another person wants to be a 
and say your intentions when buying the cappuccino were just to have this delicious coffee beverage and therefore only 
side effect that these people were how do we resolve the disagreement here now you could just say these 
just equally important and equally coherent sets of rules but i think a better approach is look at the 
function of the intuitions that they are drawing on and say is there a sort of unified framework that 
that evolutionary function that original goal of the system sc right i think we agree on a lot of 
but maybe we disagree on the metaethical question of whether moral rules are objectively real or not is that 
dl right and a lot of this hinges on what we mean by real so i think i sc 
usually does yes chuckle dl yeah exactly when i say real i mean real in the sense that smoking 
bad for your health is a fact a real fact about human beings and dependent on certain goals that 
all share now this is something that i think inclined to agree with too that if we talk about 
as a set of as the philosopher philippa foot once said hypothetical imperatives as a set of sort of 
if you want to be x then you should do y if you wanna be healthy then you should 
and eat right and so on then we can have objective answers to this question within the domain of 
all sharing these goals however i am in agreement with you sc and i completely agree with that right 
yes and so outside of these goals that we all share no way of talking about one state as 
or worse than another one and where i think you and sam harris might disagree here and on your 
of that debate he has this thought experiment that he calls the worst possible suffering for everyone where he 
imagine everyone is in total misery all the time clearly objectively bad but i think it only makes sense 
say that objectively bad if we already are within the realm of caring about suffering and avoiding suffering sc 
to me much like saying aesthetic judgments are objective because if we imagine the ugliest painting everyone would agree 
ugly therefore it must be objective i think okay to admit that contingent human beings made of atoms obeying 
laws of physics and this thing called our moral goals are very reliant on historical contingent things like different 
could have different goals but happily we agree on a lot and we can build from what we agree 
moral systems okay sorry i ended by saying we can build from what we agree some sensible moral systems 
yes i totally agree now the question is is which of these internally consistent and all apparently intuitive different 
of rules should we select from and going to have to make choices like that because as you said 
constructing a selfdriving car going to have to decide which paths that result in collisions are better and worse 
others and it would be easy it would be really wonderful if we could just avoid all collisions that 
make my job completely unnecessary and i am fine with that if it turns out that we can avoid 
kind of harm to anybody then we need ethics at all but every time the machine is going to 
evaluating one path and comparing it to another going to have to decide which collisions are better and worse 
a lot of ways of doing it it could for instance consider the driver and the passengers of its 
as more valuable than the other passengers it could consider swerving to hit someone as better or worse than 
straight it could view hitting and injuring more people as better or worse than fewer people dl now some 
those might seem more obvious to you or less obvious to you but the problem is how do we 
resolve this and i think we need a moral theory and for that like i was saying we need 
framework of comparing these theories and i think the only way of doing that is by saying which moral 
actually fulfills this metaethical assumption that moral theories evolve for the purpose of promoting cooperative behavior among selfinterested organisms 
you have a moral theory that actually is better at creating cooperative behavior than other kinds of theories then 
one we should use i propose in designing these vehicles sc yeah maybe good to clear up this issue 
moving on to the specifics of the different moral theories there is a subjection out there to this kind 
discussion that says cares cars are not really gonna be solving trolley problems if they see something bad just 
hit the brakes and stop and so this is all kind of irrelevant and i suspect just an impoverished 
of how moral reasoning works and people get annoyed with philosophers for inventing trolley problems because they say i 
wanna play this but a way of sharpening our intuition right and actually it would be useful here if 
laid out what a trolley problem is conceivable that some of the audience know and you also you mentioned 
the book that harry decision to drop an atomic bomb on japan was very much like a trolley problem 
yes right so these do happen and unfortunately they happen quite often in certain professions like in medicine and 
and business where you have to make choices about causing harm to one person or allowing harm to many 
people now this sounds very strange to most people like me because i think about myself as well just 
through my day buying cappuccinos grading papers watching netflix what am i doing that is causing harm to others 
in fact if buying that cappuccino you are weighing your own pleasure against the happiness and suffering of other 
who you could be donating that money too if watching netflix you could be doing more with your time 
eating meat making judgments about whether animals are valuable or not and the trolley problem is one of the 
that was constructed in order to demonstrate the differences well initially between doing and allowing and killing many people 
one or i should say sacrificing one person to save many sc right dl in the scenario you have 
runaway train going down a track gonna hit five people and the only way to stop it is to 
divert it onto a side track where a single person is or in another condition to push a large 
in front of the train and stop the train now it turns out that most people say permissible in 
to switch the track onto the single person but not permissible to push the large man to his death 
though that seems strangely inconsistent if you are only considering the consequences then it is the same exact effect 
one to say five but if you think that actually performing some kind of physical intrusion into somebody personal 
is important or if you think about this in terms of a causal chain or your intentions or something 
that then maybe there is a difference here dl so the trolley problem is one way of comparing what 
moral theories might say utilitarians will say you always kill one to say five a lot of deontologists what 
have in common aside from just focusing on rights and duties is that they often say that nothing wrong 
just standing back and allowing things to happen so this is called the difference between a positive and a 
obligation the philosopher robert nozick made a big deal about this and said wrong of me to push you 
front of a train not wrong for me to allow you to be hit by a and so according 
most deontologists wrong for me to push you in front of a train and it might also be wrong 
me to pull that switch causing the train to go on to you sc but i think that a 
helpful illuminating thought experiment precisely because how we viscerally think about morally right and wrong might not match with 
we say thinking right what the pushing the guy off of the footbridge really brings home that if you 
ask someone abstractly is if given two choices would you let five people die or one person die say 
i would let the one person die but then when you make it concrete you have to actually kill 
one person to save the other five unwilling to do it and kind of okay and i think irrational 
revealing that our moral intuitions are not always very coherent or matching up with our moral cognition dl yeah 
as someone taught ethics classes to undergrads for a long time very familiar with how sort of inconsistent everyday 
intuitions are if you press just a little bit on some of these trolley problems like okay to pull 
switch and in fact you should pull the switch to kill one to safe five but what if it 
your mother or your father or your best friend on the side well then people change their minds and 
you asked them why do you think okay for you to value your parents over other parents surely these 
are equally valuable not saying that your family is actually better than other but it turns out that moral 
actually are sensitive to this there was a fantastic experiment done by some researchers led by april bleskerechek and 
found that alternating by condition the genetic relatedness of the person on the side track will alternate whether people 
willing to pull the switch almost exactly as you would predict by the proportion of their genetic relatedness so 
cousin and so on sc what is the famous joke i would sacrifice my brother or sister to save 
cousins or something like that dl yeah exactly i think that was the quote was it herbert spencer or 
sc yeah right dl and so but most moral theories agree that genetic relatedness should not play a role 
in fact if you press people and ask are you saying that people who are genetically related to you 
better than the people who are not genetically related to you say of course not silly i would never 
but by acting in that way they are revealing that judgment is actually playing a role in their behavior 
yeah and i think that again part of it is that a lot of people have a presumption whether 
or implicit that the right kind of ultimate moral theory will be something utilitarian but maybe not maybe perfectly 
or at least say not maybe but say i can imagine a perfectly coherent moral theory that very explicitly 
more credit to people who are closer to me when i come to saving lives dl yeah exactly the 
has to say some really really weird things but the other moral theories also sometimes have to say some 
really weird things and this is what you have to get used to is that no consistent set of 
is gonna give you everything that you want all the time and my theory that advocating also tells me 
things that i really like and i think is really weird chuckle sc why we give you a chance 
explain what your theory is which is not completely yours building on quite a tradition here dl right i 
advocating a moral theory that is drawn from a tradition called contractarianism and the most recent version of this 
using was from a philosopher an american philosopher named john rawls and in his book a theory of justice 
he proposed that the best way of designing a fair society is to imagine that we were in this 
position where i know who going to be i could be anyone and in this kind of idealized bargaining 
he thought we would all come to agree on certain basic distributions of what he called primary goods and 
distinction between primary goods and secondary goods are that when you know who going to be you know if 
to be male or female tall or short handicapped or perfectly abled and so on and if the case 
know what kinds of particular things gonna value are you gonna like television are you gonna like coffee maybe 
not these are all secondary goods primary goods are the kinds of things that all human beings value no 
what that all human beings have to value in order to pursue any kind of goal at all dl 
this list includes things like your life your health your opportunity and essential resources for survival so no matter 
you wanna do with your life if you wanna be a juggler if you wanna be a lawyer if 
wanna be a physicist you need to have essential resources opportunities and health right sc yep dl going back 
our previous discussion these are the kinds of things that we have as you could call them common ground 
all human beings as a matter of fact just by virtue of being human beings care about and if 
says i value these things i could say yes you do a human being and you pursue goals and 
you care about your health and safety and opportunity sc i actually took a class with john rawls in 
school dl really sc yes it was very funny well i audited the class but i went to those 
and everything and i remember one day walking with a friend of mine across campus and the people who 
actually taken the class for a grade were coming out of the final exam and i just ran into 
and so i said you know how did the exam and they were like very chuckle sc which of 
my friend who is like that must not be your physics friends those must be your philosophy friends because 
physics people have ever come out of an exam saying that was very fair but his whole thing was 
as fairness and trying to make things as fair to everyone as possible dl right and rawls is primarily 
as a political philosopher because the kinds of things he was talking about designing from this original position were 
policies and the structure of our government and social institutions however towards the end of the book he talks 
little bit about using this as a frame work for individual decision making and what i want to be 
i wanna say that we can also use this as a way of thinking about what kinds of actions 
wrong and what kinds of actions are permissible from the original position rawls said we would all agree on 
certain distribution principle that he called the maximin principal and the maximin principle has a history from game theory 
in this context it just means we would agree on a distribution which makes the poorest person as best 
as possible sc yeah actually i do wanna i get into this but i realize now while saying this 
prior thing i wanna just touch on very quickly you mentioned the fact that i think that rawls himself 
have cast his theory as a political one right a way to organize our society in fact only a 
society he admits that there would be cases where things were in extreme distress where have to violate his 
but the idea as i understood it was that we could disagree on basic moral conceptions but if we 
to live together in a liberal democratic polity then he had these rules for how to reconcile our different 
conceptions and so going a little bit farther because you wanna actually use this as a theory of morality 
well right dl right well i wanna say that there are many kinds of values that are equally comparable 
each other however those all exist within a kind of space that is constrained by essentially this moral decision 
so there are lots of equally good distributions according to the maximin principle and within that space of equally 
distributions of primary goods then we could go ahead and impose many different kinds of values and have interesting 
about which sets of values are the ones that are best but importantly all occurring within the space of 
sort of maximin constraint sc but it is a little asking you a bit more right because the original 
is something where we forget some things about ourselves like you said and we remember other things ourselves the 
between primary versus secondary goals and this seems potentially more problematic if we want to get out of it 
rules rather than just a political system if someone is in real life very religious and has some religious 
that strongly flavor their ideas of right and wrong are those convictions things they will have to forget in 
original position and dl yes sc that lead them into something a moral theory coming out of the original 
that is very different than the one they actually have dl probably yes but i think we need john 
to convince us that religion is irrelevant to ethics i think just some basic assumptions about what we mean 
making moral choices can do that and this goes back to the dialogue euthyphro by plato where he plausibly 
that look even if god were to say that slavery and child abuse and rape are morally good that 
them good and so usually when talking to somebody who thinks that morality is based on a certain set 
religious beliefs it takes about ninety seconds of talking to them to get them to finally admit yeah i 
right that it really matter what the religious text says what matters is something sc i think that on 
with i can never pronounce it euthyphro dl euthyphro yeah sc euthyphro dilemma and why there should be some 
for morality other than what god says but nevertheless i think that i could imagine that the actual moral 
that a religious person has are different not because god gave them the moral beliefs but because their religious 
affect how they think about the world how they think about the ontology of reality if you believe that 
life begins at conception you might have different views on abortion than if you believe just a bunch of 
obeying the laws of biology dl yeah interesting i think the abortion case is difficult and in fact people 
when talking about ethics jump right to abortion which is one of the most complicated moral topics and i 
like to point out basically starting off at the introduction of the book and just skipping right to the 
complicated problem at the very chuckle dl right a lot of stuff that goes on in between and most 
that we face are actually i think ones that are plausibly ones that have good moral answers to them 
as i mentioned slavery child abuse rape and then of course we get to more difficult cases like charity 
eating animals and driving a fossil fuel vehicle which i think are in fact things that most people think 
may be morally permissible or wrong and are probably mistaken about and then we get to very very difficult 
like abortion which even if there is not an answer to that and i think there is probably an 
to it but even if there is not that invalidate everything that sort of went up to that point 
okay but just trying to get on the table the idea that when we get back to the selfdriving 
killing people solving their little trolley problem as going down the street i could at least imagine that people 
deepseated moral convictions that qualify in as conception as primary goods and they might object to having those conceptions 
away of them as we put them in the original position dl i think correct but i also think 
that any group of people who are interacting with each other are going to have certain beliefs that are 
respected in the process of interacting and so if we are having a civil society together just to take 
political case then inevitably some of your beliefs are probably going to come into conflict with other belief and 
the institution so if i have a religious objection to say oh i know respecting other people of different 
then going to say as a government no you actually have to treat everybody equally and it matter what 
religious beliefs are sc right yeah no i think that the reason just i harp on this is become 
very interested in this potential conflict between fundamental moral positions that individuals might have and the goal that we 
share of living in a liberal democratic society and i think that we tend to paper over them these 
a little bit but i think we should respect that they could be true conflicts and have to eventually 
something like what you just said which is that suck it up some people are gonna have to have 
make compromises if gonna live together like dl yeah i totally agree and i think instead of phrasing it 
would also say suck it up but sort of the pittsburg in me chuckle dl i think that a 
congenial way of phrasing that is that you might have very different religious values from me but there are 
set of values that we all share and what we need to base a moral theory on is the 
of universal grounds that we all have in common the kinds of things that enable all human beings to 
their goals sc right a much more public relations friendly way of putting the points chuckle sc so i 
a wise way of doing it okay i know you wanna get to the maximin principle so do i 
maybe even before we do that i really liked in your book the casting things in terms of game 
and prisoners dilemmas and nash equilibria and pareto optimality and all these other buzz words and i think that 
is why we have an hour long podcast so we can actually explain a little bit how thinking a 
helpful conceptual tool so why is game theory something that is useful tool to have in mind when we 
about these issues dl yeah i am really excited to talk about this because i think this is the 
forward in resolving these kinds of tensions between different consistent moral theories this kind of tool was only available 
philosophers for the last sixty years or so a lot of times when i tell people that i work 
designing moral frameworks for machines say philosophers been doing this for thousands of years and they gotten now my 
response is just because a problem has been around for a long time mean it be and the second 
is i think the tools for solving these kinds of problems have really only emerged in the last fifty 
sixty years or and when we talk about evaluating theories based on which one promotes cooperative behavior been a 
of talk in the history of philosophy about promoting cooperation the british philosopher thomas hobbs talks a lot about 
we have any kinds of rules we would need to invent ones in order to cooperate you and i 
been talking about living in a civil society and cooperating together but exactly what does this mean when we 
about cooperation well there is a very very technical way of describing this and you can describe it as 
kind of improvement from simple selfinterested behavior dl selfinterest is actually a very powerful tool and in the john 
described a certain method of describe of showing how selfinterested agents would come to certain equilibria in interactions and 
with each other and of course games just mean poker and blackjack but it can just mean any situation 
two or more people are interacting and there are gains and losses for those people sc but poker was 
big influence right that was a big inspiration chuckle dl right well it also includes games too chuckle dl 
so what talking about here are cases say the dilemma is usually phrased in terms of sort of cops 
robbers drama where say that i arrest you for drug dealing and i know dealing drugs but i have 
evidence to convict you now i make you a deal you and your partner a deal in separate rooms 
say if you will confess to the crime let you off free but gonna put your partner away for 
you know if you both stay quiet that you actually get say a very low sentence if both of 
confess you both get a medium sentence and it turns out in this kind of scenario a lot of 
might think intuitive that you should stay quiet but according to nash you should both confess you should both 
on each other sc right dl now weird about this is got a conflict where it turns out there 
an improvement if both of you confess to the crime then you both get a medium sentence but if 
of you were to stay quiet you would have both gotten a low sentence now called a pareto improvement 
it is improvement for everyone it make anyone worse off and these are the kinds of improvements that economists 
are the bare minimum for rationality like if i finish my lunch and hungry and sitting next to me 
got some leftovers it seems obvious that i should just give some to you it make me any worse 
and it makes you better off a pareto improvement from the italian economist vilfredo pareto dl and so defining 
problems as ones where selfinterest here and selfinterest measuring as a nash equilibrium in fact there are lots of 
situations that have different kinds of nash equilibria you could have multiple nash equilibria but there exist pareto improvements 
nash equilibria sc if i understand it correctly the nash equilibrium is one where one person cannot unilaterally change 
get a better outcome without hurting somebody else but a pareto improvement would be where if we all change 
once we will all be better off is that right dl exactly right and the challenge is that pareto 
from these nash equilibria are not things that selfinterested rational agents can do not capable of it and sc 
a cooperation problem dl exactly a problem and this is the challenge that thomas hobbs in the was describing 
that people in their own selfinterest are going to be led to these outcomes that are actually not optimal 
everyone and so we need this sort of thirdparty as he called it the leviathan or maybe a set 
rules or a government to come in and force us to act in a way for mutual benefit sc 
as you just alluded to being pareto optimal is kind of something that nobody could disagree with right not 
the final answer to our right thing to do but if there is something where if everyone acted in 
certain way literally everybody would be better off or at least the same then how could anyone object to 
right dl exactly and the kind of thing that you would expect in the evolutionary history of our species 
other species as well would motivate certain adaptive traits to emerge would actually lead certain traits to emerge to 
us to cooperate in places where we have cooperated before sc and you go on to propose the repugnant 
chuckle as an illustration of how straightforward utilitarianism can lead us wrong and kind of a version of the 
monster thought experiment that i guess was it nozick who proposed that dl yeah sc and where if one 
can become way better off and everyone else suffers just a little bit straight forward utilitarianism would say sure 
everyone suffer just a little bit this one person would be so much better and you would argue that 
not the moral strategy we wanna pursue dl right the great thing about defining cooperation in this very formal 
is that we could actually go on and test which of our moral theories produce more and less cooperative 
i think that in most cases like the dilemma the regular dilemma it turns out that utilitarianism contractarianism natural 
theories kantian ethics they all produce the correct result they all produce mutual cooperation which is great and i 
our moral intuitions this sort of mixed bag of cognitive mechanisms that have over time evolved to make these 
of choices that they also in most situations do a great job of promoting cooperative behavior sc except maybe 
does not get the same answer for the iterated dilemma dl maybe yeah it depends on how you define 
the harm this is something that nozick talks a lot about in anarchy state and utopia which was his 
in the to rawls he said that if talking about causing harm as sort of doing something where if 
had done otherwise she would have been worse off then in this case maybe the dilemma is not an 
of causing harm to the other person if you confess or if you stay quiet but difficult to say 
that scenario really what counts as causing harm sc but nevertheless i think i take your point that for 
conventional dilemma we have a nash equilibrium where everyone defects but most sensible people would say that both players 
the game are better off if they cooperate and so we can have that as a starting point of 
and work from there dl yeah and even more than most sensible people over time if you are a 
or a contractarian playing dilemma over and over and over again you will get better outcomes if measuring this 
money make more money over time if measuring this in children have more children over time sc right okay 
gonna finally let you tell us what a good contractarian believes you mentioned the maximin principle but how is 
different how would a good contractarian approach something like a dilemma or other sorts of games differently than a 
utilitarian would dl right the maximin principle says that we should prefer the distribution that makes the worst off 
as best off as possible and usually what that means is you have a set of outcomes you attach 
to each of those outcomes number values and in each of the outcomes you pick the worst off then 
put those into a group and select the highest of the lows and the one you pick now in 
of distribution of money fairly straight forward how you count and quantify those distributions but in terms of other 
of goods it might be a little more complicated however utilitarians have been spending years decades centuries to try 
convince us that pleasures and pains can be quantified and counted sc and added up dl and added up 
and so the utilitarian wants to run essentially a summation function over all of this and just pick the 
of the sums now those usually produce very similar answer so in the dilemma it turns out that adding 
all the outcomes and running maximin both say that we should cooperate with each other sc right dl but 
other scenarios you could arrange it and you mentioned the repugnant dilemma that i set up you could arrange 
where the sum of all the payoff for people is actually not either what maximin would say nor is 
what pareto optimality would predict over just selfinterested behavior and for that reason i think that the maximin principle 
actually the better principle than the utilitarian one sc and precisely because of this possibility that there could be 
gains for one person but other people have to suffer because of it dl exactly and almost a prediction 
the theory not necessarily a motivation for it the real motivation for it is that it produces cooperative behavior 
all scenarios the prediction is that it will make the worst off as well off as possible usually this 
a lot of sense intuitively in a like you said a liberal democracy very often progressives are wanting to 
the poor before we benefit the rich that they should have priority so this is often called a prioritarian 
however there are some other situations where it be prioritarian i think where the rubber meets the road here 
when you start actually attaching values to outcomes and you have to do that by saying here are the 
goods what i was saying earlier are the kinds of goods that all human beings from the original position 
care about our health our safety our opportunity and you try to quantify them and then calculate the effects 
your action on those goods sc yeah dl so if i say look if going to punch you in 
face i might get some amount of pleasure from that if some sadistic weirdo i want to do that 
if i did it would be terrible in terms of your health and opportunity and that loss to you 
primary goods is not equivalent or not made up for the gain to me and the secondary goods that 
get namely my pleasure or something like that and so when we talk about applying this to selfdriving cars 
we need to do is we need to have a way of quantifying the effects of every collision on 
health and safety of the passengers of pedestrians of people in other cars and then what a contractarian would 
is run a maximin function over all of that and say are three different collisions what are the worst 
outcomes in each of these and going to pick the best of the worst case scenarios sc so that 
like something sensible but just before we dig into that i think safe to say that in the political 
where rawls was originally talking about his reasoning does seem to lead us to quite redistributive way of running 
the very worse off people have to be improved by any inequality that we allow so a very different 
than where we actually live where in modern capitalism in the united states plenty of people suffering a lot 
the idea that other people who are doing really well off because of economic growth and what a tradeoff 
to make dl right now if utilitarian you care very much about the suffering of these other people so 
use the word suffering but if a contractarian i care about their suffering i care about the distribution of 
goods namely their health their safety their essential resources and so what i care about is making sure that 
worst off people in the population are brought up to a minimal level of just call it normal functioning 
a lot of discussion about this in bioethics about what is normal functioning how do we quantify normal functioning 
essentially what a contractarian is trying to do to bring everybody up to a minimal threshold of opportunity and 
but not happiness in fact contractarians care about happiness happiness is not the good that we are calculating sc 
but for rawls certainly wealth that an individual has would be among the goods that we do calculate right 
we talk about gains dl sure sc the difference principle saying that inequality should only be allowed to the 
that everyone is better off wealth is among the things that makes us better off dl sure but only 
the extent that wealth is able to get you the essential resources you need to pursue goals if you 
a masochist and you enjoy suffering fine as long as you have enough essential resources to continue being a 
then all a contractarian cares about sc sure right just trying because when we get to do the selfdriving 
there will be competing conceptions of what the car should be doing so i just want people to know 
analogous competing conceptions in the political arena rawls he in at least at face of value would be much 
democratic socialist whereas a libertarian would be much more capitalist in terms of how the economy should run itself 
these are both plausible theories that we can argue about dl right yes sc good when we come to 
cars going to try to implement some kind of maximin algorithm in the mind of a selfdriving car dl 
so i think there needs to be a database of collisions and the effects of these collisions on most 
of comparable say size and position right now this is something that you might think is really really complicated 
even maybe a little bit silly but i think the alternative is even sillier right now a lot of 
major car companies have the official position of just saying we think all collisions are bad and we want 
avoid them all but i think an incredibly ridiculous position to take because not all collisions are equal obviously 
hit by a vehicle moving at two miles an hour is better than getting hit by a vehicle moving 
twenty miles an hour and i want vehicles that are evaluating different paths to say that one collision is 
than another sc yeah no i even quite understand the resistance to this way of thinking if someone says 
the best economic system and someone else said the system where everybody is that would not be very convincing 
anyone like not the that we have to make some hard choices here we should at least anticipate the 
that cars are gonna be making some hard choices dl i think the reality that is slowly coming but 
think sort of a public relations nightmare for an industry that is already working hard to just convince people 
these things are safe at all much less to convince them that they should be evaluating which kinds of 
are better and worse sc and there is i think that you made this point in the book that 
sunk into my brain before reading it which is that neither you the human driver nor the car the 
intelligence can say with perfect certainty what the outcome of a decision is going to be therefore even if 
that someone actually gets run over a car will constantly will be making decisions between higher risk and lower 
actions and that is really quite down to earth and gonna be common right dl right talked to a 
people who are designing autonomous systems mostly in academics not in the industry industry wanna talk at all about 
kind of stuff chuckle dl and i can understand why but a lot of the people in academics who 
working on this technology i talked to for instance benjamin kuipers who was building along with his former postdoc 
jin park they built this wonderful autonomous robot that moves around the halls of the university of michigan and 
obstacles and slows down and tries to avoid them and park used this system called model predictive control where 
essentially casts out a net of many many many possible paths many per second and then it prunes those 
based on the likelihood that each of them is going to result in a collision now likelihood is a 
great method to evaluate paths i want to take the paths that are least likely to result in collisions 
once again i think we need more than just likelihood i think we also need to say a likely 
with a pedestrian is worse than a likely collision with a tree sc right yeah exactly how simple and 
forward does this suggested algorithm become when it comes to things like trolley problems or things like babies versus 
or anything like that i mean there still seems to be a lot of wishywashiness there dl yeah so 
will tell us what kinds of information is relevant in making this database in the first place which i 
is really important there was a recent experiment conducted by the mit media lab that probably familiar with it 
just published in nature a couple of weeks ago and it was called the moral machine experiment sc right 
dl and so what they did is they asked people to make choices about selfdriving car trolley problems where 
alternated things like the gender the age the social status of all the people involved so would you rather 
over two doctors and a homeless person to save one obese man and a dog or something like that 
the contractarian as well as most moral theories are going to say all of that information is irrelevant or 
of it is irrelevant so whether a person is a doctor or a lawyer whether a person is a 
or a christian or an atheist all of that is irrelevant but what is relevant are things like your 
position your physical size and maybe your age because that information actually tells us about the effects of this 
with you and so this is important in figuring out what kinds of databases are gonna be discriminatory against 
and what kinds are not sc well you brought up this very interesting question do you discriminate against people 
motorcycles who are wearing helmets versus those who are not because presumably a collision with someone wearing a helmet 
hurt them less than someone not wearing a helmet so we actually punish them in some sense dl yeah 
one of the more counterintuitive predictions of my theory my theory says a lot of things that i find 
intuitive but a few things that i find counterintuitive and unfortunately if my theory is correct i just have 
say so much the worse for my intuitions part of the problem might be the use of the word 
so a little bit misleading if the car is going to evaluate a collision with a bicyclist without a 
as worse than a collision with a bicyclist with a helmet that mean that it hates the one with 
helmet or that it thinks that the one without a helmet deserves to die more than the one without 
helmet only saying this path is less dangerous than that path and the reason why i agree with you 
seems really weird is that you think well the person with the helmet was being safe the one who 
the house that day taking precautions why should she have the car target her or punish her more than 
other one and the answer is i think we need to stop using words like target or punish and 
say that the path that leads to you was evaluated as better than the path that led to her 
okay good and so i think yeah two big looming questions not quite clear on here yet but i 
we can clear them up one is you seem to be saying that the contractarian just treats every human 
equally roughly speaking maybe some health differences like maybe a strong person be as bad to get an accident 
as a weak person because more likely to survive it but this is contrary to how many intuition goes 
of the aspects of the mit study if i remember correctly was that different people from different parts of 
world gave different answers for injuring women versus men young people versus old people etcetera but saying that advocating 
ignorant over all of that dl right so if people are preferring to collide with men over women my 
would be sexism and not something we wanna incorporate into our machine sc right good some people are not 
agree with this gonna have to try to convince them but okay dl well yeah i have to keep 
back until we find some grounds that we could agree on step back one step and say well what 
theory are you using and in just about any moral theory not going to value men more than women 
viceversa not utilitarianism not kantian ethics nothing and if they say they still do well i take a further 
back and say well how should we even make decisions in the world right should we just base off 
the things that we all have in common and if agreeing on that then going to say contractarianism is 
best way of cooperating based on the values that we all share sc okay what about babies versus grown 
dl babies versus grown ups is difficult because a grown up and when just talking about collisions is more 
to survive a crash than a baby and so in that case the baby should be preferred but not 
we love babies more or more adorable but because they are more vulnerable sc okay that makes sense good 
then the other looming issue was be explicit about how we come down on the various trolley problem kind 
scenarios here it sounds like contractarianism really care if one person versus five gets injured because of an active 
versus a passive one right it is a consequentialist point of view at the end of the day dl 
right right i do think we need to evaluate outcomes based on the distributions they produce and that is 
kind of consequentialism and so in that way i think utilitarianism and contractarianism are sort of cousins in this 
but i think the biggest difference is their way of quantifying the goods do they quantify happiness and suffering 
primary goods and do they run a summation function or a maximin function and so in say the trolley 
in most cases going to agree but in some cases gonna disagree and in some cases i find it 
weird dl one of those cases according to my theory this is something that a friend of mine the 
susan anderson pointed out she pointed out that according to maximin it would be better for the car to 
into a crowd of fifty people and give them all a broken leg rather than to swerve into a 
wall and give the passenger in the vehicle two broken legs why is that because fifty single broken legs 
better than one instance of two broken legs and i find that so strange i find that crazy but 
again i just have to say just like jeremy bentham did in the my theory says it so i 
to accept jeremy bentham was talking about homosexuality and he said to my theory i guess all even though 
to him it was really weird and gross he had to accept it sc well i agree with what 
consequences are sometimes when our moral theories give us highly counterintuitive or weird sounding suggestions we need to say 
i have a wrong moral right dl well actually something that rawls thought so rawls agreed that we need 
go through this process called reflective equilibrium where we sort of tune our own intuitions to the theories that 
are developing however where i think i would diverge from rawls i would say that look if a matteroffact 
which actions create more cooperative behavior than others then just like if the doctor tells me to stop smoking 
i really really want to i have to say look a matter of fact which actions are right and 
or which actions are healthy and but i could still say i want to do that or even not 
to do that however still a fact of the matter about what the right thing to do is sc 
but we do need to make some choice about whether or not discovered that fact of the matter through 
moral theorizing or whether or not we should be a little bit less confident that chosen a function utilitarian 
a function over all utility which is to say add it all up and maximize it and you in 
sense chosen a function over utility which is to say at the utility of the worst off person and 
right and maybe some happy medium so that the fifty people get their legs broken dl i see how 
would work although open to thinking about it so there are a lot of people who want to have 
versions between these two but once again the problem is if you wanna mix the two theories together i 
you need to have a third theory that tells you when do you take the utilitarian choice when do 
take the contractarian choice and i just know what that third theory would look like sc well you say 
now when the fifty people are suing you because they all have their legs broken you might feel differently 
dl oh no say that yeah actually someone of the conference recently joked to me that if wildly successful 
my dreams and this actually was used in selfdriving cars i could be responsible for millions of injuries and 
and i laughed because i know that not going to happen but there was a part of me that 
of was a little bit afraid i mean i know that the car companies need to work out some 
of solution to this and the problem is not talking about what doing sc yeah you could equally well 
that if you succeed beyond your wildest dreams be responsible for saving enormous amounts of death and suffering in 
world right dl sure sure let me ask you if you mind i assume sort of taking the more 
approach here i got from what you were saying that generally you take a sort of utilitarian although utilitarian 
from your big picture book but still more or less good old fashion utilitarian approach to most kinds of 
like this sc actually no just trying to give you a hard time because just trying to figure out 
the right thing to do is i really have strong substantive moral theory myself i believe in utilitarianism because 
totally buy the utility monster kind of responses or the repugnant conclusion derek parfit had this very similar argument 
it would always be better just to have more children just have more and more killed kids because there 
be more and more people having happiness and extraordinarily skeptical of the idea that we can number one calculate 
utility for people maybe possible but then number two add them up on some commensurable scale seems like the 
thing to do to me so almost to the point where willing to accept some kind of deontology rather 
some kind of consequentialist way of thinking but not quite sure what that would be dl interesting so you 
two objections to utilitarianism one of them is that it match your intuitions on some weird cases and the 
one is that just very very hard to implement hard to calculate pleasures and pains sc right dl now 
a utilitarian i might say as to the first one so much the worse for your and in addition 
might point now being a utilitarian for some reason by the way sc yeah chuckle dl but i might 
point out that any moral theory is gonna say really really counterintuitive thing so not sure why we should 
if a crazy sounding scenario with an alien where it seem to match our intuitions do you expect that 
to be a moral theory that matches all of your intuitions at some point sc no but i actually 
buy point on reflective equilibrium because as a moral antirealist i think that getting our starting point for morality 
our moral intuitions as a cognitive realist i understand that those intuitions might be incoherent and therefore work for 
philosophy to do from our moral intuitions building them into the best fit sensible logical coherent system but yeah 
think evidence when the system that tried to build is wildly in conflict with the intuitions i started with 
might be either because i gotta get rid of that intuition or because i did a bad job building 
system open to both possibilities dl i think fair i think fair i think my main concern about using 
and evaluating the theory is that so aware of the history of strong intuitions that have been false sc 
dl that i just give them virtually no evidential weight whatsoever in addition to that a lot of intuitions 
have right now that i suspect almost any moral theory is gonna tell me wrong like i said i 
the taste of meat but any plausible moral theory is gonna tell me that if i can lead a 
healthy life without eating meat that i really should i love driving fossil fuel vehicles i love it chuckle 
but most moral theories tell me that if that has terrible effects on the environment and i really need 
be doing that i live in a city i have public transport so again most moral theories are gonna 
me things i really wanna hear and very sensitive sc yeah no i definetely agree we have to be 
to throwing out this or that moral intuition or at least dramatically changing it and i think this is 
makes human beings pretty cool is that we only have our moral intuitions where we start but we also 
our rational cognitive capabilities and we feedback we can go from rationality to alter our moral feelings and it 
happen like a meateater and i drive a fossil fuel car i wanna get rid of the fossil fuel 
but not gonna get rid of eating to meat but be happier if we could make artificial meat and 
to kill any animals to do it dl sure i could see that and the problem in appealing to 
or rational sort of corrections here is that if nothing outside of our intuitions that appealing to to correct 
then not on sure how we escape the inevitability of an internally coherent system just completely mistaken sc yeah 
necessarily believe that the word mistaken has any reference there in the world i think dl yeah i can 
that i think then just coming to blows about whether we think the function of moral theories is to 
cooperative behavior among selfinterested organisms or whether to produce sort of satisfying solutions according to our contingent intuitions that 
all happen to share or some of us happen to share sc yeah i think that in both senses 
to be coherent and rational either individually or collectively interesting to me is that people have strong disagreements about 
realism versus antirealism and those agreements are almost entirely uncorrelated with their ideas about what actually is and is 
moral chuckle dl that is really fascinating to me too i find that in talking to most sort of 
people in my friend circles that they are explicitly moral relativists but implicitly utilitarians sc interesting yeah dl yeah 
usually like you said sort of good utilitarians where willing to sacrifice one person to save many but then 
also want to sacrifice cappuccinos and fossil fuel and eating meat and so on and so if you push 
far enough maybe admit that explicitly but then they might fall back on relativism and say all or something 
that so relative when convenient sc yeah for me utilitarianism is an example of something that i reason to 
out of as far as concerned i think that it sounds superficially the right thing to do but i 
the objections to it are good enough that looking for something better dl yeah just in case curious i 
gonna bring this in i have a survey that was conducted by the website philpapersorg run by david chalmers 
his group and they asked professional philosophers do you accept the category of normative ethics described as consequentialism deontology 
virtue ethics sc right dl and roughly split as accept or lean deontology and some change accept or lean 
consequentialism towards virtue ethics and other and this actually reminds me of a poll that you took and you 
in one of your blog posts about your survey of interpretations of quantum mechanics sc yeah not a lot 
consensus dl yeah not a lot of consensus and you called this a huge embarrassment for the field of 
and i kind of feel that way about my own field in some ways i must admit i feel 
it is a little embarrassing that these are not just theories that are sort of fun to think about 
they actually make a difference in how we live and how we design artificial intelligence and it turns out 
there is not a lot of consensus in the field where there should be sc do you have the 
there for moral realism versus antirealism dl i actually might yeah hold on a second sc that was also 
philpapers survey question i remember that dl yes i do sc and i think that most philosophers are realists 
dl right accept or lean towards realism anti and then other sc alright it is interesting i think more 
that we have a consensus on quantum mechanics quantum mechanics should be easier than ethics or morality but more 
that we have a consensus on ethics or morality dl right where the analogy might end is that most 
the versions of quantum mechanics if i understand it make essentially similar predictions however the moral theories although one 
say in of cases most of the moral theories probably make the same predictions just these rare scenarios of 
involving say opportunity cost what you could be doing instead of what doing right now that the biggest and 
important disagreements sc sure and if in that crowd of fifty people going at their legs broken extremely relevant 
you chuckle dl yeah sue me sc your car is this programmed one way or the other just to 
it up put a bow on it i guess we glossed over a little bit about the implement ability 
this plan you sketched out a database idea where we would have all these different possibilities how real world 
this prospect of making contractarianism the way that our selfdriving cars go about making moral decisions dl yeah a 
question and my answer is i know but if you are working on this kind of technology out there 
would love to hear from you i want to know how plausible is it to be able to design 
vehicles and other autonomous systems that can quantify the effects of these actions on primary goods and then run 
functions over them talked to people in the field who say it seems like this might be i see 
job as saying if we are going to design autonomous systems what they need to be capable of doing 
if they are not capable of doing this then we should slow down or maybe even halt the development 
this technology sc right right dl and i think that is especially relevant in the domain of autonomous weapons 
sc well good here are my final two questions which could be short answers or longer but one of 
is you brought up an issue in the book that again i was surprised because i even thought of 
is it a problem if an artificially intelligent system does things that seem to be ethical to us but 
articulate why doing them and this is an issue for deep learning systems where they can recognize a picture 
it tell us why it recognized a picture a human being would be able to articulate an answer that 
not be the correct reason why they did something but at least they can try should we expect the 
from ai dl a great question and the answer is not sure there is a kantian position here that 
that not a real decision getting back to the very first thing we talked about not a decision responsible 
unless you can actually articulate the reasons for it you can tell me why you did it otherwise just 
of an animal or a child acting on instinct now to me it so much matter if you can 
it what matters is are you following the maximin principle sc right dl and i think the best way 
doing this is actually constructing the maximin principle in these autonomous systems in called a topdown approach however also 
to the possibility of what you might call approximating a maximin principle through these more bottomup methods if the 
learning system produced outputs that always matched the maximin principle in the kinds of cases we observe and we 
good reason for thinking that it would continue to run this program that approximated maximin in future cases then 
would say that would be say close to good enough or sufficient in that case sc i think so 
think a little bit too much to demand of our ai systems that they be articulate moral philosophers chuckle 
long as they seem to be doing mostly the right things dl right well as long as it says 
like the reason why i chose this path instead of this path is that the worst collision in this 
is better than the worst collision in that path it need to say something like i traveled into the 
position and i realized from there that maximin was chuckle sc yeah be too much to ask and the 
final question was something you already alluded to a potential difference between the every day life circumstance of a 
driving around and trying to avoid accidents and the every day but not life case of people at war 
machines that were intentionally built in order to inflict harm in a certain way how do the moral considerations 
and i realize this is a huge topic but maybe a simple introduction to the differences between that and 
day life in wartime dl well the smallest case is that this might be applied in right now are 
you could call security robots sc right dl and in fact these are currently being used in some airports 
china and other places in east asia where they have in some cases taser technology equipped with them and 
there are good things about this kind of technology and in fact if someone is harming another person it 
good if a robot could step in and actually neutralize the threat but the problem is in doing so 
needs to be capable of identifying when there is a threat what kind of threat this is and what 
proportional amount of force is to neutralize that threat and so contractarianism does make predictions about this if you 
quantify the kind of harm being done by that threatening agent that enemy agent and you could say usually 
neutralizing the threat is better than just say killing the agent because that would be certainly making the agent 
worst off sc yeah dl but neutralizing the threat would be the best of all possible outcomes and so 
could imagine security robots and in the extreme military robots being designed with their goal of neutralizing enemies and 
threats because i think that would be the maximin approach to it sc right but maximin seems maybe just 
conceptulalizing it correctly but it seems to fail us a little bit when literally our goal is to kill 
dl right and i think that you could imagine cases and i do imagine cases where the ideal autonomous 
in war would be commanded to kill an enemy soldier and the robot would say thank you but i 
going to apprehend him and take him into in fact the goal going back to our good friend of 
kant he famously and shockingly said god commands you to kill your own children the correct response is not 
to do and we tell people in military ethics and the ethics of war that if your commanding officer 
you to kill innocent people in war the correct answer is but i am happy to do other things 
are not war sc do i remember correctly that in the book you suggested or at least wondered out 
whether or not it might be okay to ultimately have autonomous selfdriving cars or drones and so forth but 
in the theater of war that autonomy should be always in the hands of human agents who actually can 
responsibility dl right there was a letter that was recently signed by a number of people who work on 
and political philosophy and these group of people were arguing that autonomous systems should not be used in war 
all now i go that far but i would agree that the kinds of capabilities they would require in 
to i wanna use the word responsible but in order to make the right choices in war are unlikely 
happen any time soon and so all of my claims are a big hypothetical which is if we are 
to design these kinds of machines these are the kinds of capabilities that they would require and willing to 
that for military robots as well with more skepticism that they are actually going to achieve this level of 
than in the case of medical technology or transportation technology sc very interesting to me because i see a 
version of what happens in physics in particular in science more generally where concepts that we could have ignored 
earlier times are forced to the forefront of our attention by the progress of technology right and so i 
a wonderful thing for philosophy that our discussions about morality are being sharpened a little bit by the fact 
we be wishywashy we be fuzzy about them we just say be excellent to each other we need to 
machines that will listen to us quite literally how to behave in a wide variety of circumstances dl yeah 
i have to admit a friend of mine pushed me on this position that i take he said it 
kind of bullshit what doing because if i was taking a strong moral stance against autonomous weapon can i 
bullshit on your program sc absolutely dl okay so that kind of bullshit and that i am being hypocritical 
not really caring about the use of this technology that in fact just saying going to build it the 
way of doing and sensitive to that objection very sensitive to it not convinced that autonomous weapon systems or 
vehicles or autonomous medical care bots are actually a good idea in the long run and sensitive to the 
that maybe this position taking is a little bit too corporate but that being said if any corporations would 
to pay me large amounts of money i am more than available sc very good well i do hope 
take you up on that certainly on your side in thinking that this is something where we should face 
to their problems rather than ignore them derek leben thanks so much for being on the podcast dl thank 
sean music sc if i understand it correctly the nash equilibrium is one where one person cannot unilaterally change 
get a better outcome without hurting somebody else but a pareto improvement would be where if we all change 
once we will all be better off is that right dl exactly right this is not it has nothing 
do with hurting other parties irrelevant you are in a nash equilibrium when any change to your strategy you 
enact will be counter to your interests payoff function in the prisoner dilemma game you defect because it matter 
your adversary does you still benefit either he defects too in which case you avoid a long sentence or 
stays quiet in which case you get the lowest sentence to do otherwise it to guarantee you cannot get 
lowest sentence and open yourself to the possibility that you may get the highest i see the value in 
deeply about programing machines so that there are ethical guidelines to prevent unnecessary harm however when i am driving 
car if i recognize that a collision is about to occur my instinct will be self preservation i think 
car manufacturers will recognize this human tendency and will present these to the public as designed to maximize personal 
it is doubtful that buyers will trade their own autonomy for a machine that might have a different priority 
many times a year does a human driver have to think if she should run into a bicyclist wearing 
helmet or her friend who in order to swerve to avoid a ninety year man who just walked in 
of her of accidents have nothing to do with the trolley car dilemma yet this was about of the 
alex in the prisoner dilemma game you defect because it matter what your adversary does you still benefit either 
defects too in which case you avoid a long sentence or he stays quiet in which case you get 
lowest sentence that is inaccurate in most versions of the dilemma when repeated over many iterations the nash equilibrium 
both players defecting which leaves both players worse off than both cooperating any strategy other than defecting leaves the 
side open to exploiting your strategy and becoming much better off the dilemma describes a nonpareto optimal nash equilibrium 
deciding where to extract natural resources to sell them also effects people in a similar ethical dilemma there are 
other illustrative examples of the problem that are not self driving cars that is just a popular topic at 
time as autonomous vehicles are beginning to affect our lives the trolley square problem is an analogy for many 
ais will not specifically about running over people or dogs or flowers or beep boop but people constantly running 
the trolley problem when they drive so why would a driverless car in the extremely rare instance when this 
happen it as if the human driver usually has time to think of the action to take between two 
outcomes so why would we expect the driverless car to do better i liked the rest of the podcast 
in the discussion of various philosophical theories for the programing of robots it is important to realize that they 
not understand anything it is not possible to program them to understand therefore we need to restrict their functions 
areas where understanding is not required perhaps understanding is not required in the driving of automobiles or semitrailer trucks 
crowded roads and neighborhood streets but we need to make sure they solved selfdriving cars some years ago they 
called trains not a big fan of the absolutism of what actions are and harm if one was trying 
create an algorithm of harm they would have to consider the impact of the action as well the impact 
society stealing fifty dollars from the president for instance as likely to harm the president as a uneducated poor 
trying to take a train to work at the same time the president is much more valuable to society 
the uneducated poor person now have to find a way to mesh what i call the moral value and 
value in my above example despite the president being probably millions of times more beneficial to society the harm 
losing fifty dollars would be basically and so the uneducated poor worker who would be harmed greatly by not 
able to get to work would end up being harmed more in the combined value another issue about the 
about what is being talked about is with the trolley problem when we switch the track over we actually 
that is going to kill someone heck we even know pushing someone in front of a trolley is going 
kill them our actions all lead to a probability someone is harmed or killed you would likely have to 
the probability of harm weigh the extensiveness of it and mesh that into the combined value as well sean 
state at that you do not fully buy into utilitarian arguments in part due to the and criticisms i 
both of these responses have been addressed within utilitarian circles utility monster i argue that this criticism is not 
bug but a feature of utilitarianism working correctly in the thought experiment we are asked to imagine an individual 
of generating greater utility than thousands of other individuals combined observe that utilitarianism demands we devote significant resources to 
that one even if this comes at great expense to the many we are left to conclude that two 
a flaw in utilitarianism the true issue is that limits in human imaginations result in many people struggling to 
we often fail to envision a human who could act as a utility monster in relation to other humans 
are left wondering makes them so this is a flaw that is smuggled into the framing of the thought 
in situations where the utility monster truly is accepted as being capable of generating greater utility than the many 
results are not so unintuitive a single human is a utility monster in relation to ants a single ant 
a utility monster in relation to bacteria a single bacterium is a utility monster in relation to grains of 
these conclusions are not nearly so controversial repugnant conclusion repugnant conclusion presents a very strong argument against attempting to 
utility through the implication is that a massive group of miserable people is greater than a small group of 
who are very content for this reason i am more supportive of calculating utility using a mean arithmetic or 
geometric as a side note i personally appreciate that this approach does not imply that adding more people is 
desirable as advocated by former guest tyler cowen upon applying his reasoning to this model parfit reasonably points out 
a very small number of extremely high utility people possibly just one person would be preferable to billions of 
who produce even slightly less utility on average again the discomfort generated by this conclusion is due to lapses 
imagination we are unable to easily imagine a world where a single individual could reliable generate higher average utility 
a large society even if we could there would be meaningful moral implications for any plan to transition from 
current situation containing billions of moral agents down to very few a frequent criticism of averaging utility functions is 
claim that utility could be increased by killing everyone who is generating low there is a very reasonable response 
this criticism a moral community includes all people ever to exist be they in the past present or future 
if committing genocide could lead to greater average utility amongst those who remained the utility of those who were 
must still be factored in one would be unlikely to achieve a net gain in average utility after accounted 
the negative utility generated during the mass slaughter interested in alternative utility aggregation functions beyond utilitarian sum of utility 
maximin it seems like there should be a large number of such functions to consider some which may have 
tradeoffs better alignment with intuitive preferable choices one which i like is sumsqrtu this seems to strike a good 
to me preferring to increase the utility of the lowest but also providing some reduced guidance for all population 
my feeling is that pragmatically these ais will seek to optimize legality and minimize liability with no consideration of 
in some ways a less interesting discussion and just defers the problem to the legislative process it also opens 
about whose liability should be optimized the system operatorowner manufacturer comments are closed sean carroll hosts conversations with the 
most interesting thinkers science society philosophy culture arts and ideas 