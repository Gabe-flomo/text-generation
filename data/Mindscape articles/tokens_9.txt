
intelligence has made great strides of late in areas as diverse as playing go and recognizing pictures of dogs 
still seem to be a ways away from ai that is intelligent in the human sense but it might 
be too long before we have to start thinking seriously about the and of artificial agents stuart russell is 
longtime expert in ai and he takes extremely seriously the worry that these motivations and purposes may be dramatically 
odds with our own in his book human compatible russell suggests that the secret is to give up on 
our own goals into computers and rather programming them to figure out our goals by actually observing how humans 
support mindscape on patreon stuart russell received his phd in computer science from stanford university he is currently a 
of computer science and the smithzadeh professor in engineering at the university of california berkeley as well as an 
fellow of wadham college oxford he is a cofounder of the center for humancompatible artificial intelligence at uc berkeley 
is the author of several books including with peter norvig the classic text artificial intelligence a modern approach among 
numerous awards are the ijcai computers and thought award the blaise pascal chair in paris and the world technology 
his new book is human compatible artificial intelligence and the problem of control web pagegoogle scholar publicationswikipediatalk on provably 
artificial intelligenceamazon author page click to show episode transcript click above to close artificial intelligence has made great strides 
late in areas as diverse as playing go and recognizing pictures of dogs we still seem to be a 
away from ai that is intelligent in the human sense but it might not be too long before we 
to start thinking seriously about the and of artificial agents stuart russell is a longtime expert in ai and 
takes extremely seriously the worry that these motivations and purposes may be dramatically at odds with our own in 
book human compatible russell suggests that the secret is to give up on building our own goals into computers 
rather programming them to figure out our goals by actually observing how humans behave support mindscape on patreon stuart 
received his phd in computer science from stanford university he is currently a professor of computer science and the 
professor in engineering at the university of california berkeley as well as an honorary fellow of wadham college oxford 
is a cofounder of the center for humancompatible artificial intelligence at uc berkeley he is the author of several 
including with peter norvig the classic text artificial intelligence a modern approach among his numerous awards are the ijcai 
and thought award the blaise pascal chair in paris and the world technology award his new book is human 
artificial intelligence and the problem of control click to show episode transcript sean carroll hello everyone and welcome to 
mindscape podcast your host sean carroll i hope everyone is more or less staying home and staying safe in 
surreal times that in with the quarantine lockdown etcetera very challenging out there for everyone as said frequently probably 
so for me than most people since i basically work from home and i have a pretty comfortable situation 
but different you get to see your friends get to socialize get to eat out get to go to 
in the same way sc one thing that struck me is how really different it would have been if 
like this happened before we had computers and the internet of course plagues and quarantines and pandemics did happen 
just the idea that we talk to each other over zoom or send emails or get updates i think 
would be much worse it would certainly be very very different so appropriate that we once again turn to 
world of computers in this episode of mindscape and in particular the challenge of artificial intelligence we all know 
ai is becoming more and more important not just as some hypothetical future thing that will talk to us 
be our friend but right here right now programs that figure out puzzles in a way that you could 
of as being intelligent sc now a looming problem here that everybody knows about which is as the ais 
better and better as they become smarter and smarter more and more powerful going to become increasingly important that 
act in ways that are what we want them to do intelligent they can figure out things to do 
themselves it would be bad if what they wanted to do in some sense and we can figure out 
extent to which the word want is appropriate in the case of a computer but what their goals are 
the same as what we want them to be and this can be very difficult because as anyone ever 
a computer knows it is very difficult to say what you want it is very difficult to put into 
algorithmic terms what it is looking for it why computer programming is hard sc and so very easy to 
asking the computer asking an ai to do a certain thing and then realizing at the end of the 
we really want that we wanted something that was that but there were some constraints that we forgot to 
so guest is stuart russell a very big name in artificial intelligence stuart and peter norvig wrote what is 
classic textbook on ai called artificial intelligence a modern approach even i have this book on my bookshelf and 
really an ai person so that shows you the pervasiveness of that particular text sc and recently stuart has 
thinking about exactly this problem making sure that ais sort of do the things that we wanted them to 
when we designed them so he has a new book out called humancompatible artificial intelligence and the problem of 
and to really oversimplify it his proposal is that rather than telling the ais what we want we will 
the ais to learn from us what we want maybe we even know what we want but maybe the 
can watch our actual behavior and from that work backwards to how they should be behaving to do what 
is we want now this opens a whole bunch of other questions so what we talk about on the 
how realistic this is how much we should be worried about superintelligent ai and what it means for the 
more generally sc so anyway i hope that the mindscape podcast is contributing a little bit to taking our 
off of the situation in with the pandemic i want one alternative would be to switch all efforts into 
about pandemics and epidemiology and things like that but i figure plenty of people with more background knowledge than 
am doing those things so going to try to keep mindscape more or less the same kind of thing 
always has been i think still important to keep thinking about other questions pandemics and virology are part of 
medicine and networks and how people communicate is all part of that those are interesting things to talk about 
sure we will but also be talking about physics and biology and philosophy and culture and the media and 
forth sc remember that also doing a set of videos called the biggest ideas in the universe that are 
on youtube once a week once a week i do a big idea i was really hoping to make 
around twenty minutes per idea crept up to more like an hour with me talking and scribbling little pictures 
very primitive equations but i hope people are finding them fun so far it has been more or less 
ideas like space and time and things like that but soon enough be getting to some pretty wild ideas 
i think not going to be speculating too much when i choose the ideas themselves not going to be 
string theory or loop quantum gravity or whatever but some of the ideas that are part and parcel of 
physics are pretty wild even the established ones so be getting into those and i hope you enjoy those 
and with that go music sc stuart russell welcome to the mindscape podcast stuart russell nice to be here 
going to talk about artificial intelligence the warnings the worries that we have about artificial intelligence and also the 
of it but i think that be a wonderful opportunity here since you not only are someone who has 
ideas about fixing any dangers of artificial intelligence but also someone who has been doing research in this field 
wrote the textbook etcetera to talk about some of the background here so i thought i would just start 
asking you in your mind what does the phrase artificial intelligence actually mean how do you define that term 
so obviously artificial intelligence is about making machines intelligent and what that has turned into actually from right at 
beginning of the field is an approach to building systems that are given objectives by humans and then find 
to carry out those objectives so the objectives might be goals can you find a plan that achieves this 
it might be a reward function can you behave in a way that generates the largest amount of reward 
example can you make money on the stock market so this has been the way that the general idea 
intelligence has been instantiated to build systems that act in ways that can be expected to achieve the objectives 
we set for them sc and is there some way of definitively saying when a certain computer program is 
versus just a regular computer program in some senses a pocket calculator optimized to add numbers together correctly sr 
and i think part of the difficulty there really is a continuum between something as simple as a thermostat 
switches the heat on when it gets cold and switches it off when it gets warm all the way 
to humans and even beyond and the continuum is mainly in the nature of the task environment how complicated 
the world that the entity has to deal with and then what is the actual objective and some objectives 
be achieved even in a complicated world can be achieved with very very simple rules and some for example 
at a game of go or running a country or teaching someone to speak french these are very very 
goals and they take place in a very very complicated task environment which is an environment that includes human 
and probably the most complicated environment we know of is the one where there are humans that are part 
it sc but you would say i like the point that a continuum here not like there is some 
phase transition between dumb computer programs and artificially intelligent ones sr right in fact nostrum been put about a 
time that as soon as it works it stops being artificial intelligence sc oh not heard that one sr 
is a little bit unfair of course that means that ai is the field of continual failure but i 
in some ways accurate that a lot of things that ai has produced over the for example every time 
drive in your car and you get directions an ai algorithm running in your cell phone or maybe in 
cloud that is computing shortest paths and incorporating expected delays and so on this is a very classical ai 
that was developed in the late and early and no one thinks of that as ai anymore but of 
someone had to invent it and before it was invented no one knew how to do it and we 
i think take speech recognition for granted but another part of ai been gradually improving for the last fifty 
or so but now that it works and just one of the things that runs on your cell phone 
think of it as ai anymore sc there sr and a pity because it also means that a tendency 
think for students to not want to study things that we already know how to do which means of 
that the students learn how to do anything sc that works right chuckle but even you mentioned playing go 
one of the more complicated tasks but i would sort of put playing go more on the finding a 
to your destination side of the ledger and driving a car or writing a poem on another side if 
think about the fact that the rules of go are extremely explicitly defined and even though the state space 
huge the space of possible things that can be going on still finite right there is a right answer 
every question whereas these more human scale things are a little bit fuzzier is that a relevant distinction you 
sr it is yes i think multiple distinctions being made here one is that as you say the rules 
go are known so the objective is to win and that has a precise definition that varies slightly from 
to country but pretty much agreed on and you know that when you put a piece on a square 
piece ends up on that square it sort of run around by itself and go somewhere else and of 
when driving you even know the rules you know going to happen when some other entity is involved in 
decisions about where going to go which will affect your own ability to get what you want done done 
the other part of driving difficult is that the objective is also not known so obviously you would like 
you say me to the you would like to get to the airport but you mean me to the 
at all so if an earthquake and the bay bridge falls down again then that mean me two hundred 
around and get me to the airport an hour after my flight it mean plunge into the bay and 
to swim across the bay sr when you give a human an objective like me the the human understands 
what circumstances is it reasonable to give up on that plus there are many different ways of getting to 
airport is it that you take the shortest route is it that you take the fastest route and then 
fast do you drive how safely do you drive do you go right up to the edge of catastrophe 
all times in the hope of getting to the airport a little bit quicker taken taxis where that was 
they drove and taken other taxis when you just want to say can you just get on with because 
are extremely cautious so there are many ways of driving and there an actually agreed on objective for what 
as good driving and how that depends on the circumstance and the urgency and so on sr so driving 
different from go in both of these dimensions and an unsolved problem right now whereas i think we would 
that the go to the extent that it can be solved and there are mathematical reasons to believe that 
probably solve it exactly but to the extent that we can come up with good algorithms we consider the 
of go to be one where ai has had success beaten the human world champions and actually taught humans 
about the game that they know before sc so you mentioned an idea that i think is worth sort 
lingering on and drawing out going to be important for the rest of the conversation the idea of optimizing 
some goal right probably if you told a person on the street that what an ai program was trying 
do they might not instantly see the connection between ordinary human intelligence do you think that we can think 
all intelligence as optimizing towards some goal or maximizing some utility function or something like that sr a good 
and i would say that taken a few thousand years to convince at least one branch of the intellectual 
that a good way of thinking and the branch most convinced about that would be the economists because reached 
the they reached a formulation of rational decisionmaking that is quite general so it allows for the decisionmaker to 
uncertainty about the state of the world about the future it does at least in the original formulation assume 
the decisionmaker has preferences and this is the way economists think about it that if you think about each 
future that could unfold in principle i could express a preference as to which future i like better than 
other future and the basis for then the economic notion of rationality as utility maximization it mean that you 
to have a number in your head which is the utility of each future and then you have to 
them all up and divide by the probabilities and so on sr it simply means that you act as 
you did and you can be as long as not making mistakes then your behavior can be described as 
what you were doing it mean that you are actually doing that so for example when if i try 
poke myself in the eye with my finger my eyelid closes now my brain is not doing an expected 
calculation to reason that i ought to close my eyes because having a fingernail poking my eyeball is painful 
dangerous and that would be a low utility and so if i close my eye then protect it no 
thing just an instantaneous reaction to a looming object in the visual field but still a rational thing to 
in other words if you were to do a expected utility calculation it would tell you to close your 
but you have to do that calculation in order to close your eyes sc visualizing thousands of podcast listeners 
to poke themselves in the eye as we speak right here sr yeah max tegmark said it work he 
to poke himself in the eye and he succeeded sc yeah well as someone who used to wear contacts 
think have no no trouble with doing that but this is good because i think i do and this 
probably leaping ahead and okay we need to go in logical order but you quote a theorem in your 
you wrote a wonderful book called human compatible i not just saying that as boilerplate i really enjoyed the 
and i encourage everyone to take a look at it sr thank you sc so a theorem by von 
and morgenstern which if i understand it correctly heard of it before and then was reintroduced to it in 
book roughly that conclusion you just said that if behaving rationally then always a way to cast the decisions 
in terms of maximizing some utility function is that right sr yeah so it starts from a description of 
we mean by behaving rationally pretty hard to argue with and how these kinds of proofs usually go you 
some axioms that sound completely reasonable and then you draw what appears to be a very strong conclusion so 
axioms say things like you prefer sausage pizza to plain pizza and you prefer plain pizza to pineapple pizza 
you prefer a sausage pizza to pineapple right this is called transitivity of preferences and what that basically says 
your preferences go around in a circle and that seems perfectly reasonable because if your preferences did go around 
a circle then basically you would pay me to go in the other direction so if you liked sausage 
than plain and you had a plain pizza pay me to get the sausage pizza and then if you 
pineapple better than sausage you would pay me to get the pineapple and then if you liked pineapple better 
plain so your preferences are now in a circle pay me to get the plain sc so now paid 
three times and back where you started so that seem very rational so the axioms of rational behavior have 
flavor that really hard to argue with have to agree that an entity that follow those axioms would basically 
shooting itself in the foot all the time and then the consequence of these axioms is that the behavior 
a rational agent can be described as if maximizing a utility function sc good yeah sr and so that 
the theorem of von neumann the main theorem in that book sc but i think also already hinted at 
little bit of a tension there because lots of mathematical theorems that say something is possible in principle but 
you already mentioned with the poke in the eye example for actual systems that do something we call reasoning 
might not implement that design in order to get things done but is it safe to say that most 
algorithms do try to implement something like that that they do actually sort of optimize over a utility function 
a whole bunch of hypothetical actions sr yes and no the biggest obstacle from the point of view of 
is that actually optimizing in many situations is what we call computationally intractable which roughly means that if you 
want your algorithm to come up with the optimal solution going to have to run it for until the 
ends so billions and billions of years of computation and then if you make the problem twice as big 
to be a billion times more than that so these kinds of problems which we call computationally intractable we 
expect that any entity is going to be able to behave rationally if it has to solve those problems 
order to get what it wants sr so humans behave rationally and we expect that machines will behave rationally 
interesting question is how much closer to being rational can machines get than humans particularly taking advantage of their 
computational power so already we have computers whose actual computational throughput is larger than even the theoretical maximum throughput 
the human brain and no one thinks that in fact the human brain really is able to use every 
neuron computing at maximum speed all time sr so the interesting question is are machines going to exceed the 
capability for decisionmaking in the real world and when will that happen and by how much will it happen 
interestingly humans have developed over both evolutionary time and during their lifetimes all kinds of incredible schemes for overcoming 
computational difficulty of the problem of deciding what to do and so even though not particularly good at playing 
and in fact a lot of games are specifically designed to not be easy for our brains our brains 
actually extremely good at organizing our behavior into hierarchies sr so if writing a book say at any given 
your fingers are typing one character of one word of one sentence of one paragraph of a chapter of 
book and this is a natural hierarchy just typing one character on the keyboard might involve hundreds of motor 
commands from your brain to your arm and your hands and your finger as well as your eye to 
track of happening on the screen if you think about what alphago is doing which is the go program 
beat the human world champion able to look ahead in time maybe fifty moves into the future which sounds 
amazing but if you apply that to a robot or a human body where those fifty moves would be 
motor control commands to my muscles that would only take you about a of a second into the future 
so it would be completely useless for real decisionmaking in the real world and humans of course we think 
quite a bit further than a tenth of a second into the future sometimes even years we plan to 
a phd which is five or six years in other words a trillion motor control commands are involved in 
a phd so planning at many many levels of abstraction and we do it kind of seamlessly somehow miraculously 
brain always has something lined up for the present moment so that if speaking your tongue and your lips 
saying the next word if typing the next word is coming out of your fingers if going for a 
the right things are happening for your legs to move in alternation sr and yet the same thing is 
at the scale of multiple seconds and minutes and hours and days and weeks and it all feels seamless 
aware of sort of context switching oh now i need to think about happening in the next millisecond oh 
going to think about happening an hourandahalf from now all very smoothly integrated and we really know how to 
machines to do this but this is the main technique that humans use to manage the complexity of the 
world sc yeah a really great summary of a whole bunch of podcast interviews that done with people who 
neuroscience and cognitive science and consciousness studies and absolutely the message comes through that human beings are these complicated 
of subsystems each using little heuristics to get through the day rather than some perfect reasoning machine that is 
to play a very abstract version of go so but at the end there you said and not very 
at converting that into an ai algorithm is this something that people are trying to do is this a 
difference in the approach of conventional ai and where we eventually want to be sr so something that known 
in fact herbert simon who was both a psychologist and an economist who won the nobel prize and an 
researcher one of the early ai researchers who really set some of the directions for the field wrote a 
famous paper called the architecture of complexity where he specifically talks about this device of hierarchical layers of abstraction 
help us to organize our behavior in time early planning systems going back to the attempted to implement some 
of hierarchical planning capability so there is actually a fairly well understood ability if i give the hierarchy to 
system in other words if i tell it okay these are the primitive actions and then these are the 
that use primitive actions to be implemented so this is remote control command for your fingers this is typing 
character this is typing a word this is typing a sentence we have algorithms that can construct longterm plans 
people have used this to for example organize the manufacturing process in a factory for a month so that 
plan actually bottoms out at something in the order of six hundred million manufacturing operations that are going to 
place over that month sr the difficulty is that those hierarchies those abstract actions have to be provided by 
human so we are doing a lot of the work one of the big missing pieces in ai is 
does a system develop that hierarchy and invent those abstract actions for itself there was a time when getting 
phd exist there was just no such thing emailing someone googling someone even a few years ago those exist 
have a library of these things at all kinds of levels of abstraction from multiyear things like getting a 
or emigrating to the united states down to much shorter term things like googling something or emailing someone or 
a lot of these things actually we invent individually gradually formed somehow by cultural processes and we inherit them 
in some sense then the intelligence really in the individual human something that inherited from our culture and incredibly 
without that vocabulary without that library of abstract actions life would be intolerably complicated and we simply be able 
have the civilization that we have because it would be too difficult to navigate it and to run it 
we had to think of everything in terms of just the primitive operations sc yeah you give an example 
the book about i forget exactly the context but there was a question a bunch of fourth graders are 
to have a roller skating contest what is the best kind of surface that they should choose to move 
gravel sand black top or whatever and you talk about how much background knowledge is necessary to even think 
such a question like what does it mean to have a roller skating contest what is the goal that 
fourth graders have and so forth you seem to say in the book that right now our computers are 
bad at that background knowledge i had a conversation with melanie mitchell on the podcast where she also argues 
the lack of common sense in our ai algorithms so again is this something where everyone knows this in 
and working hard and we think we can crack it or is there a school of thought that says 
just come eventually if we keep working on what working on now sr so a great question known it 
a long time there are papers going back to the saying that absolutely particularly in the case of understanding 
that we need this background of common sense knowledge in order to make the right decisions and for a 
time a field called knowledge of representation where people tried to actually capture the knowledge in mathematical form and 
in mathematical logic and computer algorithms are able to process logic in order to do reasoning and we can 
learn these logical representations from examples people found it quite difficult to get it right so for example writing 
description in mathematical logic of the process of making omelettes turns out to be pretty difficult to get right 
you can capture some parts of it with precise logical statements but other parts of it are quite resistant 
to say what you mean by not runny but not too overcooked and things like that sr so i 
say that effort is kind of on hold right now there were valiant attempts to build up largescale knowledge 
with lots of common sense but those have not turned out to be all that useful in practice so 
now the ai community is working largely on approaches based on deep learning and in deep learning systems no 
to actually construct representations of knowledge no attempt to put in common sense at all we simply take data 
we feed in the data and use the data to train very large tunable circuits to perform various tasks 
you can put in image data and train these circuits to recognize the objects that appear in the image 
having trained an image recognition system on millions of photographs of animals all the system can do so now 
animal recognizer but it in some sense know anything about animals sr so you ask questions like well which 
these animals could pick up a suitcase totally outside the scope of what that large trained circuit is able 
do and at the moment i think there are any of these machine learning systems that are capable of 
those kinds of common sense questions you can make it appear as if they do and this is another 
would say failing of the current of the current trends in ai you can just put in millions and 
of questionanswer pairs like that and then you hope that if i give it a new question that something 
enough in the data set that it can cobble together some answer that is right most of the time 
a japanese researcher noriko arai who has been able to train a machine learning system to pass the entrance 
for the university of tokyo which is a very difficult and selective exam sc pretty good yeah sr most 
in japan obviously fail to do because the university of tokyo is the premier university but she freely admits 
the system has absolutely no idea about anything it understand any of the questions it has no knowledge to 
of just been trained on enough examples and enough background text data that able to spot enough statistical correlations 
what the words are in the question and constituents in the question to what the answer should be but 
reasoning process going on at all so my guess and then i think people like demis hassabis the ceo 
deepmind and one of the authors of the alphago system so demis has said that deep learning is going 
hit a wall basically and going to have to go back and reintegrate all of the things that classical 
worked on the symbolic techniques the logic reasoning knowledge representation and so on and the question is how does 
integration work how does deep learning get integrated with these kinds of symbolic techniques and whoever figures out the 
to that question is going to be able to make the next set of breakthroughs in ai sc the 
for our listeners absolutely this is great because we have a lot of prospects and also worries about artificial 
and how similar it is to human intelligence on the table so therefore just skip ahead to the question 
should we be worried or concerned or optimistic about the prospect of artificial superintelligence given that artificial intelligence is 
very good at understanding the questions on the tokyo entrance exam should we be worried about ai number one 
there a prospect that it will become much smarter than human beings and number two is that a prospect 
should be worried about sr so these are two really important questions really hard to think of more important 
that we could be facing so on the first question are we going to be able to create superintelligent 
systems i would say that we will be able to solve these obstacles i think there are half a 
major obstacles major breakthroughs that have to happen between here and humanlevel ai and i already mentioned one of 
which is the ability of systems to form their decisionmaking hierarchies which allow them to cope with complexity and 
if you think of that sort of one breakthrough as far as we can tell about half a dozen 
that scale that need to happen it might be that once we have all those breakthroughs the system still 
because something that we think of but i think at that point be pretty close sr and we also 
to remember it have to be more capable than humans along every dimension in order to be something that 
have to worry about so just to give you an example chimpanzees it turns out have considerably better shortterm 
than human beings even for something very human like a string of digits so chimpanzees i think can remember 
numbers without too much difficulty and humans but i think chimpanzees still have good reason to worry about human 
capabilities because we are basically in the process of destroying them with the capabilities that we have sr and 
fact for many species on earth the existential risk that they face is entirely a consequence of another species 
earth namely us that is more intelligent than they are so of course reasonable to ask if we create 
that are more intelligent than us is that a good idea in particular are we going to be able 
retain control retain power over entities that are more powerful than we are hard to find good examples in 
of a less powerful species having power over a more powerful species but what being asked the problem being 
to solve sr so i think the public discussion of this topic is confusing a lot of different things 
one is is superintelligent ai imminent in other words do i need to be worried right now that something 
is going to happen right now and the answer is no ai systems at least not in the existential 
of a risk to humanity no there is no risk to humanity but bad things are already happening with 
systems even ones that are really really really stupid because they run on billions of computers particularly the algorithms 
run on social media that select what you see what you read what you watch what you listen to 
hours every day for billions of people those algorithms i think are already having a serious negative impact on 
world sr another question is well should we be worried right now that something bad is going to happen 
the future and i think the right question to ask because the timelines that most people think about are 
the order of decades and actually a little more conservative i would generally say well i expect human level 
of seriously risky ai capabilities more towards the end of the century rather than midcentury which is where most 
researchers think that have them so entirely reasonable to be concerned about that just as if we discovered a 
undetected asteroid that was quite likely to hit the earth in sixty or seventy time sr it will be 
reasonable to be worried now about that future event and actually we are already worried about that we already 
a planetary defense agency part of nasa they are busy tracking all these asteroids trying to figure out what 
we do if we detected a sizeable one that really was on a collision course and so far we 
one on a collision course but so far the statistics show that only detected about of the objects that 
going to come close to the earth and are large enough to be a civilizationending event so be too 
even about that risk sr now the risk from ai is a little different from an asteroid because the 
of an asteroid and why it would be a bad thing are pretty clear and straightforward whereas with ai 
little harder to figure out exactly how things might go wrong the same would be true if you asked 
chimpanzees a few thousand years ago okay so got these humans they hear much smarter than you how is 
all going to end the chimpanzees would have a very hard time figuring out how it was all going 
end for them in some sense it really matter once you cede control of the world and your environment 
everything else to another species that really care about you then getting into precise details about how exactly it 
ends seems sort of beside the point sr the point is not cede control to a more powerful species 
we can help it so the sense in which i think reasonable to be concerned investing billions of dollars 
hundreds of billions of dollars pushing forward a technology with no plan for what to do when we succeed 
well yeah okay i mean that was great and a lot going on here so just trying to figure 
which direction to attack in first i mean you mentioned that just overwhelmed with artificial intelligence programs doing work 
us already none of them are anything that you would call superintelligent and so there is obviously a risk 
from not understanding what doing but a different risk than i think that highlighting here at the end in 
more intelligent species than us can we be a little bit more definite about things like species here i 
how do you imaging this superintelligent ai existing being embodied is it someone literally writes a program in c 
is it sort of emergently arising out of the internet is it in a robot is it distributed in 
cloud what should we be visualizing sr so i certainly think that the typical sciencefiction depiction for obvious reasons 
that in the head of a single robot and i think a huge mistake sc sure sr extremely unlikely 
how things would go so much more likely that it would be a composite system drawing on computational power 
the entire cloud or a good fraction of it some people would say that the negative consequences could emerge 
the unanticipated interaction of many many different systems and already seen this in the socalled flash crash which was 
a lot of trading algorithms on the stock market somehow interacted in a way that we still really understand 
then in the course of about twenty minutes they wiped a trillion dollars off the value of the stock 
and we had to shut the stock market down and we had to undo all those trades fortunately the 
market had these builtin circuit breakers which allowed the intervention to take place but when that kind of thing 
it has an effect in the real world it really shook the confidence of investors in the reasonable operation 
the market and it could have been much worse sr so these unanticipated interactions are a possibility but i 
the failure mode that we anticipate is a consequence of the standard model of ai whereby we make this 
machinery and we put in objectives and then it turns out that the objective is incomplete or incorrect in 
way this is not a new story right we go back to king midas who said want everything i 
to turn to so the objective that he put into the machine so to speak and he got exactly 
he asked for and then he realized now his food and his drink is turning to gold and his 
is turned to gold and then he dies in misery and starvation sr so the standard model is in 
i think an engineering mistake a way of creating systems that relies on perfection on the part of humans 
our ability to specify objectives and a really bad idea to assume perfection on the part of humans sc 
the thing to rely on yeah sr particularly when the thing putting an objective into is potentially more powerful 
human beings are so it could have physical appendages or it might reside in the cloud but could still 
physical appendages with which it can bring about changes in the world but it even need to do that 
you think about adolf hitler and the changes he brought about it because of his physical appendages it was 
he basically did it with words and ai systems can have impact on the world through words through communication 
the internet as they already are as social media algorithms are already impacting the world even though they have 
arms and legs and they have no laser guns and nuclear nuclear bombs and all the rest sc yeah 
i mean i think this is exactly what i want to sort maybe an unfair question but i want 
really try to drill in on the physical manifestation of this i mean there is the threat from artificial 
as you said algorithms that really do their job well or are biased or whatever sr yeah so what 
is precisely not that the algorithm is stupid that not doing its work correctly that we are specifying its 
incorrectly sc sure okay sr but it the algorithm is extraordinarily capable at carrying out the assigned objective the 
task in ways that turn out to have extremely bad side effects so if you believe possible then just 
about the fossil fuel industry and imagine the fossil fuel industry as an algorithm now it happens to have 
components that are replaceable and are replaced on a regular basis but as an algorithm trying to optimize the 
of generating a discounted stream of quarterly profits or whatever it might be and is in the process of 
the world and we are unable to stop it so this is a failure mode that we are already 
with and it can happen with ai systems just as much as it can happen with corporations that are 
an objective and neglecting the side effects sc right but somehow that i think we all agree that does 
count as superintelligent the fossil fuel industry the stupidity that i was thinking of not an ineffectualness but a 
of selfawareness selfreflection just doing its job right i think maybe misunderstanding but i think when people use the 
superintelligent they really are imagining something more humanlike right something that knows what doing and wants to do it 
not wanting to do the right thing but it has a little bit more selfcontrol and wondering would things 
that exist where would they exist how would they manifest their control in the physical world sr i think 
tendency to anthropomorphize and we see this in a number of commentators so steven pinker for example melanie mitchell 
another one who assert that intrinsic in the nature of intelligence that a sufficiently intelligent entity is going to 
in a way beneficial to human beings and i see absolutely no justification for this a priori for example 
are millions of species on earth some of them are much more numerous than human beings why the ai 
decide to be beneficial to antarctic krill probably a thousand times more krill than there are humans so why 
be beneficial to them or why not be beneficial to bacteria or why be so parochial right wrong with 
beneficial to an alien species on some star on the other side of the galaxy sr so i think 
onus is on those who believe that somehow this magic connection between the capability for rational decisionmaking and the 
of being beneficial to humans there is no such magic connection it has to be designed in and really 
human compatible is about is how do we design in that magic connection so that the more intelligent the 
system the better the outcome for human beings specifically not for aliens on the other side of the galaxy 
not for antarctic krill but for human beings sc yeah no completely on your side in thinking no natural 
toward having ai be beneficial to humans just trying to wonder how vivid can we be about how this 
would hurt us would it be just massively misdirecting the food supply would it be a million subtle interventions 
social media is it something we can even say anything about or we should just be worried across a 
spectrum of bad outcomes sr so the difficulty so i can describe scenarios and of course since i can 
them you can always say okay then just make a rule against right and then of course that scenario 
but another scenario will happen so the kinds of examples that you see commonly involve for example side effects 
modify our physical environment and this is sort of what the fossil fuel companies are doing in terms of 
carbon dioxide to the atmosphere and so you could imagine that for example you want to fix that problem 
say i would really like carbon dioxide levels to return to their preindustrial that sounds like a great goal 
a perfectly reasonable goal to state to another human being who already shares all of the other background preferences 
humans have for the future but if the sole objective then there are many ways of achieving that objective 
for example probably the simplest way is to say is all the carbon dioxide coming coming from the economic 
of get rid of the and so you say well i mean that what i meant was restore the 
dioxide levels and kill any fine okay then have a decadeslong social media campaign which convinces people immoral to 
children because those children are responsible for the gradual destruction of our planet and so we all stop having 
and the human race goes extinct and the ai system has solved its problem without killing any people and 
if anyone left to have a third wish then they come up with a revised version of that and 
the solution to that problem kills them off sr so the issue is that we noticed this problem up 
recently because our intelligent systems have been not very intelligent they are mostly in the lab mostly working on 
problems and so they have consequences on the world and if they start misbehaving we have an off switch 
switch them off in fact every robot above a certain size is legally required to have a big red 
switch usually on the back so that if it starts misbehaving you can switch it off in an emergency 
even a remote control off switch sc i did not know that sr but once you get outside the 
the effects as with social media algorithms that are directly operating on the real world can be enormous and 
also have to understand that the off switch people often say well if something really bad happens we can 
switch it but of course like saying if losing to alphago just play a better move right well sorry 
better move you might think you have alphago has already thought of it and has a way to refute 
and whatever attempt you might make to switch off the machine the ai system has of course anticipated that 
possible failure mode right one of the most obvious ways of not achieving your objective is that dead right 
fetch the coffee if dead so the system has already taken preemptive steps to prevent that failure because very 
at achieving the objectives that we give it not because it really wants to be alive sr it has 
to do with selfpreservation no instinct a logical consequence of having an objective fixed that a system wants to 
alive in order to achieve the objective sc so i guess no reason to keep the audience in suspense 
i mean the real purpose of your book is to actually propose a new strategy for dealing with exactly 
problem you think that we should be giving our ai systems a different kind of goal than we usually 
sr yes in fact you could argue we should not be giving the ai system a goal at least 
one that is precisely defined and known to the ai system because exactly when the ai system believes that 
knows the objective correctly that whatever action it comes up with in furtherance of that objective it then sort 
believes that this is the correct action to do and tolerate necessarily interference from people who are jumping up 
down saying doing that destroying the from the ai point of view just noise right just me my mouth 
open and closed so what pursuing the optimal solution to the objective that we gave it sr so the 
proposed in the book has two main parts right the first part is that we design systems in such 
way that their only objective is whatever it is that humans want the future to be like so we 
the word preferences which is the term that economists use but it just mean like preferences between different kinds 
pizza it really means out of the possible futures that the way the world could unfold in future how 
you like it to unfold and so this is what the ai system is supposed to be helping us 
but the second part of the solution is that it knows that it know what those preferences are and 
avoids the problem of a machine that is given a fixed objective and believes that that is the only 
and therefore has to be pursued at all costs sr so if we say to the ai system that 
like what doing we are conveying additional preference information to the machine and it then is rational for the 
to revise its plan because now learned a bit more about what we want and want and if it 
up with a plan that says i can fix carbon dioxide by getting rid of all the so it 
that we want to fix the carbon dioxide say it know whether the people want to be alive or 
well the rational thing for it to do if it just wants to optimize the carbon dioxide then it 
kills all the people if it knows that it know whether the people want to be alive or dead 
reasonable in fact rational for it to ask permission it says got this plan how we solve it we 
all the people and then the carbon dioxide goes back to and then you say not quite what i 
so these solutions like asking permission deferring to human instruction and then also allowing oneself to be switched off 
behaviors have to be programmed in we have to sort of put special rules and regulations into the ai 
to make it do that those behaviors are just a logical consequence of the way we frame the problem 
are rational for the ai system to do rational for it to ask permission rational for it to allow 
to be switched off because it want to do whatever it is that would cause us to switch it 
and it know what that might be but if a danger that it might do something we really like 
it would prefer to be switched off in order to avoid that from happening sc yeah i certainly like 
idea of this initial uncertainty in what it thinks the humans would prefer is this something that just by 
way have you written ai algorithms that have this feature and turn them loose and watch them behave sr 
in tiny little worlds and the mathematical description of this we call it an game so the word game 
a general term actually that economists use to mean any situation where there are multiple entities that are making 
and here we could imagine just two entities a human being and a machine and the objective is whatever 
is the human wants what i want but the machine know what that is and so you can set 
that game and for example you can make a little grid world and you can put a human in 
and although a simulated human and a simulated robot and you can solve the game you can calculate what 
the solution to this game for both the human and the robot and when you look at the solutions 
see what you hope you see that in fact the human tries to teach the robot sometimes the human 
the robot what not to do chuckle sr so basically kind of takes the robot by the hand and 
you could go over here but a big pit go in there and then you should go over here 
this is where you should sit because this is a nice place to and the robot in other versions 
the game the robot asks permission in some versions of this game the robot and the human actually devise 
little protocol where the robot kind of comes and asks a question and the human gives them a yes 
no and then the robot goes away and does the right thing so just at the beginning of this 
of exploring the nature of solutions to these kinds of formal mathematical problems but what seen so far is 
first of all the kinds of behaviors that the robot exhibits are exactly what we hope they defer they 
permission they learn from the human about what the human wants and they help the human achieve whatever it 
the human wants to achieve sr and interestingly one way of thinking about it is that this game is 
strict generalization of the classical notion of a machine pursuing a fixed objective because now not a fixed objective 
uncertain objective rather than a certain objective so strictly a more general problemsolving situation and the kinds of behaviors 
are exhibited are much richer and more interesting as a result sc it certainly bears a family resemblance to 
inference right where we have some set of priors with the probability of all of them and then we 
on the basis of more information so wondering would we expect that such a system would become less and 
uncertain over time about what it thinks the motivations are i mean do these systems grow old and crotchety 
harder to change their minds chuckle sr in a sense yes i think in a practical sense pretty unlikely 
machines would ever acquire enough information to be able to predict our preferences particularly about things that have never 
experienced in the real world so for example you can look there are whole books written about what people 
and development economists write these books and moral philosophers write these books people come up with long lists of 
are the needs of human beings but in none of those books will you find what color do people 
the sky to be and because up to now no bothered to change the color of the sky to 
out how upset people are as a result so unlikely that an ai system is going to be able 
predict our responses to changes in the color of the sky other than just sort of being generally upset 
not what it used to be but maybe like the new one better actually i know sr but also 
sort of fundamental fact that a lot we know about our own preferences so genuine what we call epistemic 
meaning that it matter how much i think about it i actually have to go and experience something i 
to get real actual empirical experience in the world in order to learn more about my preferences and a 
book by la paul a philosopher called transformative experience and she uses the example of the durian fruit sc 
just had laurie as a guest on the podcast and we talked all about transformative experiences here sr right 
independently i came up with the durian fruit as an example of this so a fruit extremely smelly and 
people absolutely love it and they think the most marvelous food on the surface of the earth and there 
other people who utterly despise it and there have been cases of where there was a cargo of durian 
an airplane and some of the passengers could not stand it so they actually mutinied on the airplane and 
the captain to go back and land at the airport and unload the cargo of durian because it was 
unbearable so clearly something that elicits either extremely positive or extremely negative preferences but i have no idea which 
i am because never tried it never smelt it so i have real epistemic uncertainty about how i would 
a future where eating lots of durian sr very similar kinds of arguments apply to much more practical kinds 
things like well imagine a school leaver would you like to be a coal miner or a librarian right 
the only two jobs we have right now lad coal miner librarian which is it going to be of 
at sixteen years old you have no idea how much going to enjoy being a coal miner or how 
going to enjoy being a librarian so you have some vague feelings about it you might have seen references 
these professions and got some images in your mind and little bits of evidence here and there but of 
you could certainly be wrong if you made one prediction or another so this real uncertainty about human preferences 
humans means that there is no practical way that the ai system is going to reach a situation of 
knowledge but the way bayesian inference works actually is that unless you make certain kinds of mistakes in the 
you set up the problem at the beginning the bayesian process only reaches certainty when correct sc yeah sr 
as long as you make sure that your prior beliefs are sufficiently broad as to encompass every possible human 
structure which is a tall order i agree but say if you can do that then you only approach 
as you approach having correct beliefs about the human preference structure so in that situation you might not mind 
much the ai system stops asking permission because it knows what going to say so it need to ask 
so it becomes sort of like the perfect butler and if read pg wodehouse and his books where jeeves 
the butler and bertie wooster is the aristocrat jeeves actually in some ways knows bertie wooster better than bertie 
himself and always anticipates what his master needs and wants and manages to do it in advance so it 
be a little bit like that but i think as i say it would be very unlikely that we 
approach any degree of certainty in any finite amount of time sc how much do we have to deeply 
about the idea of a preference i mean in the usual in the standard model i guess of ai 
you just program in a utility function that is operationally what you mean but here using the word preference 
mean something about human beings right the human preferences that are supposed to be learned by the ai if 
push it hard enough to implement in software is it even clear what human preference is i mean not 
do humans not know what their preferences are they might change they might be inconsistent right internally incoherent are 
ways to deal with this sr yes so these are great questions certainly the case that when economists or 
economists do a lot of experiments to try to figure out the actual preferences of humans even for something 
simple as money so let alone the color of the sky or what kind of pizza you want but 
money how much do you like money and risks involving money money today versus money tomorrow and i would 
that the consensus of these experiments is that consistency of preferences seems to be fairly well supported and so 
meaning that you do appear to be able to rank different outcomes in a consistent way but we exactly 
to follow some of at least straightforwardly if you interpret the other axioms of rationality we quite seem to 
those and we seem to have our own peculiar ways of making decisions sr and daniel kahneman a leading 
and another nobel prize winner in economics has studied actual human preferences and we seem to have actually sort 
two minds so his explanation is that an experiencing self which is sort of the moment to moment amount 
enjoyment or pain that experiencing and then a remembering self and in standard economic theory the remembering self is 
of supposed to add up the experiencing experiences and say well how good this was the sort of the 
of all the pleasures and pains that i but it turns out empirically that the remembering self does something 
a bit different so in particular the remembering self really remembers the most pleasurable most painful part of the 
and also the part that was most recent so they call this the peakend theory so the peak and 
end of the experience seem to be very salient in how you remember the desirability or undesirability of something 
pretty hard to reconcile with standard economic models sr but actually when you think about it clear that memory 
expectations are quite reasonably folded in to human decisionmaking because if i have a bad experience i might remember 
for my whole life and so it just the experience itself that was bad the recurring memory of it 
to be bad and that recurring memory is going to remember the worst thing so if i ask you 
the worst thing that ever happened in your probably maybe three or four things pop into your head and 
are things you think about every so often and some people even have to have to go through psychiatric 
to try to extinguish some of these memories that are ruining the rest of their lives so it might 
that in fact what the remembering self is doing is perfectly rational if you incorporate memory and expectation into 
you evaluate the costs or the benefits of some experience sr so i would say that at the moment 
are a long way from understanding the rich manylayered nature of human preferences and so on but if we 
are inconsistent so if for example if our preferences are intransitive in other words they kind of go around 
circles then nothing the ai system can do because any pizza the ai system gives you you say actually 
prefer this other and the ai system gives you that one i prefer the first and of course never 
satisfied and really nothing that any discipline of building artificial systems could possibly do to satisfy inconsistent preferences but 
can give you a pizza as opposed to no pizza at all you probably prefer some kind of pizza 
starving to death so make sure you get some kind of pizza sc we might end up learning a 
about psychology from the way that the ai determines what our preferences are in practice like which might be 
than what we say they are when we try to state them sr yeah i was going to say 
think that this is probably the conceptually the most difficult part of the research agenda that proposing is the 
between the idealized model of a human that has stable preferences and consistent preferences about the future and the 
of actual humans and i think particularly stability is conceptually very difficult to deal with if a human preferences 
change in other words what i want tomorrow to be like today is different from what i want tomorrow 
be like when i get to tomorrow then which of my two selves is the ai system supposed to 
myself today or myself tomorrow hard to pick apart that problem philosophically and then practically if human preferences can 
which clearly they do not born with a whole complex set of preferences then how do you make sure 
the ai system satisfy preferences by changing those preferences to make them easier to satisfy sc well what i 
going to say was how can we be sure that the ai just change its mind about what its 
are to satisfy the human preferences like if we really are imagining ais that are superintelligent much less just 
can we really also assume that baking in their fundamental motivational structure or is that something subject to editing 
their own part sr so this is a interesting set of questions having to do with our ability to 
mathematical theorems about the software that we build so we already have the capability to prove that algorithms are 
that they behave according to some specification and this capability is getting more and more sophisticated and we use 
even in areas like machine learning which you might think how could you possibly prove anything about an algorithm 
but it turns out that you can and you typically able to show that we expect to get a 
within some small epsilon of the desired performance and that will happen with very high probability so the kind 
theorem that we prove all the time sr so the idea here would be we would set up the 
framework for how the system operates and it would have learning components strung together in the right way and 
what you prove is that no matter how good each of those components becomes the system as a whole 
conforms to this required design where it satisfies these principles there are some tricky parts to this so traditionally 
we prove theorems about software kind of assuming that the parts of the program that we want to remain 
remain fixed and you can prove that this piece of software can never overwrite itself and change its sort 
constitution so to speak but another set of theorems have to prove which is that there is no circumstance 
which the software would convince a human being to go in and overwrite the constitution and that is much 
difficult kind of guarantee to provide sr and this comes about noticed this in computer security where we prove 
security protocols are correct but then we find out that what we call a side channel and side channels 
things like listening to the sound of the keyboard by bouncing a laser beam off the window of your 
building or measuring the electrical current in the mains supply going into the computer and then being able to 
information from these channels and when you put an ai system into the real world you automatically create this 
channel which is that the ai system can convince a human being to make physical changes to the memory 
the machine and its software and we have to make sure that that side channel is somehow cut off 
alright a lot to think about i gotta say this is a very good episode for food for thought 
just close with just giving you a chance to sort of speculate about the future what do you think 
going to be the vision of ai that comes true fifty years from now what should we be looking 
overall given all that talked about over last hour sr so assuming things go well so assuming we figure 
how to design ai systems in such a way that we retain control over them and i think this 
actually feasible a lot of very hard algorithm design and engineering work to be done to take the basic 
that described and turn it into technology that will replace and supersede all of the ai technology already out 
but assuming that works and assuming that we overcome these conceptual obstacles and achieve humanlevel or sometimes called general 
or superintelligent ai that becomes a tool that enables us to have a much better civilization than the one 
currently have our civilization is the best we can do with the brains that we have but if we 
access to much better brains even if they belong to us we get to use them and we can 
a better world than the one we have right now sr we could i think quite easily without imagining 
sort of scifi advances of eternal life or faster than light travel curing all disease but just using ai 
robotics technology to deliver the goods and services that welloff people in the west take for granted so giving 
sort of standard of life to everyone on earth would be about a tenfold increase in the gdp of 
world which when you calculate the cash equivalent value what the economists call net present value thirteen and a 
sorry thirteen and a half thousand trillion dollars so just for comparison the gdp of the us is on 
order of twenty dollars trillion a year so talking about hundreds of times larger than the gdp of the 
or in fact the gdp of the world so the cash value sr and when you look at the 
the tens or hundreds of billions of dollars that people are talking about absolutely minuscule in comparison to the 
benefits and so we could enter a golden era in a way where we no longer have to fight 
access to the wherewithal of life which is one of the main reasons been killing each other since the 
of time there are other reasons like religion that we kill each other but at least we would get 
of this reason so some people have said if we have this ai that can do everything that human 
can do and sort of could do it for us essentially for free then left for human what role 
we have where do we find a useful purpose in life if nothing that we can do better than 
the machine can do sr and i think this is a really important question for us and presumably if 
machines understand our preferences they know that we want a future of idle enfeeblement where we essentially become couch 
for the rest of time where we learn we try we do anything of any value with our lives 
sort of not what we want the future to be like so the ai systems will have to say 
some point not going to keep tying your shoelaces for you you humans have got to learn and keep 
to do stuff for so going to be an interesting discussion to have in a few time sc certainly 
us very good set of reasons to believe that we should all be thinking hard about this and trying 
figure out going to happen next stuart russell thanks so much for being on the mindscape podcast sc pleasure 
very nice talking to you music this was a fascinating conversation about the future of machine learning however it 
disconcerting to hear words describing these computers as smart or intelligent this is metaphorical language that is inappropriate for 
subject obviously these machines are able to bring a lot of value to our economy and will play a 
and larger part in every aspect of human accomplishment but they do not understand anything they are tools that 
designed and programed by intelligent humans and until we better understand the of human consciousness it is unlikely that 
will be capable of knowing or understanding any of the data that we humans feed them fascinating that ancient 
have profound relevance in examining the effects of ai midas and his golden touch as well as the djinni 
his three wishes i was trying to post a picture of the durian eating right now in thailand i 
of you now when i buy durians hahaha jameswade considering that we even say empirically whether other humans besides 
are conscious or truly understand anything we are hardly in a position to male such proclamations about hypothetical entities 
behave in ways that are perhaps more complex and intelligent than ourselves just carefully move ahead with agi research 
deal with the hard problem when we have to which may be never also i sincerely wonder if humans 
understand anything ourselves or whether just correlations all the way down comments are closed sean carroll hosts conversations with 
worlds most interesting thinkers science society philosophy culture arts and ideas 