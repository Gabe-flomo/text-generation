{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle as pk\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers import (Dense, Bidirectional, LSTM, \n",
    "                          Dropout, Embedding, BatchNormalization)\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading The Model\n",
    "What this model does is take a text document and count the amount of unique words in the document. Then the network uses the text to try and predict the unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_seq(path):\n",
    "    with open(path,'rb') as file:\n",
    "        seqs = pk.load(file)\n",
    "        print(f'Sequences loaded from: {path}')\n",
    "    \n",
    "    return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences loaded from: E:\\Documents\\My Projects\\Text Generation\\data\\HEAM.seq\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'introduction the twothousandyearold assumptionon december the deadliest school shooting in us history took place at sandy hook elementary school in newtown connecticut twentysix people inside the school including twenty children were massacred by a lone gunman several weeks after this horror i watched the governor of connecticut dannel malloy give his'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location = r'E:\\Documents\\My Projects\\Text Generation\\data\\HEAM.seq'\n",
    "sequences = load_seq(location)\n",
    "sequences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the sequences\n",
    "The word embedding layer expects input sequences to be comprised of integers.\n",
    "We can map each word in our vocabulary to a unique integer and encode our input sequences. Later, when we make predictions, we can convert the prediction to numbers and look up their associated words in the same mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Tokenizer()\n",
    "encoder.fit_on_texts(sequences)\n",
    "encoded_sequences = encoder.texts_to_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "introduction  ---- mapped to ---> [5586, 1, 10246, 10245, 10244, 1, 10243, 570, 2271, 6, 107, 644, 1010]\n"
     ]
    }
   ],
   "source": [
    "print(f'{sequences[0][:13]} ---- mapped to ---> {encoded_sequences[0][:13]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping the Encoding\n",
    "We can access the mapping of words to integers as a dictionary attribute called word_index on the Tokenizer object.\n",
    "We need to know the size of the vocabulary for defining the embedding layer later. We can determine the vocabulary by calculating the size of the mapping dictionary.\n",
    "\n",
    "Words are assigned values from 1 to the total number of words (e.g. 7,409). The Embedding layer needs to allocate a vector representation for each word in this vocabulary from index 1 to the largest index and because indexing of arrays is zero-offset, the index of the word at the end of the vocabulary will be 7,409; that means the array must be 7,409 + 1 in length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10248"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(encoder.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Inputs and Output\n",
    "Now that we have encoded the input sequences, we need to separate them into input (X) and output (y) elements.\n",
    "\n",
    "We can do this with array slicing.\n",
    "\n",
    "After separating, we need to one hot encode the output word. This means converting it from an integer to a vector of 0 values, one for each word in the vocabulary, with a 1 to indicate the specific word at the index of the words integer value.\n",
    "\n",
    "This is so that the model learns to predict the probability distribution for the next word and the ground truth from which to learn from is 0 for all words except the actual word that comes next.\n",
    "\n",
    "Keras provides the to_categorical() that can be used to one hot encode the output words for each input-output sequence pair.\n",
    "\n",
    "Finally, we need to specify to the Embedding layer how long input sequences are. We know that there are 50 words because we designed the model, but a good generic way to specify that is to use the second dimension (number of columns) of the input data’s shape. That way, if you change the length of sequences when preparing data, you do not need to change this data loading code; it is generic.\n",
    "\n",
    "# Split Input and Output Features\n",
    "It is common to split your loaded data into input variables (X) and the output variable (y).\n",
    "\n",
    "We can do this by slicing all rows and all columns up to, but before the last column, then separately indexing the last column.\n",
    "\n",
    "For the input features, we can select all rows and all columns except the last one by specifying ‘:’ for in the rows index, and :-1 in the columns index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert from list to a numpy array\n",
    "sequences = np.array(encoded_sequences)\n",
    "x, y = sequences[:,:-1], sequences[:,-1]\n",
    "# the classes are the unique words\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = x.shape[1]\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Model\n",
    "We can now define and fit our language model on the training data.\n",
    "\n",
    "The learned embedding needs to know the size of the vocabulary and the length of input sequences as previously discussed. It also has a parameter to specify how many dimensions will be used to represent each word. That is, the size of the embedding vector space.\n",
    "\n",
    "Common values are 50, 100, and 300. We will use 50 here, but consider testing smaller or larger values.\n",
    "\n",
    "We will use a two LSTM hidden layers with 100 memory cells each. More memory cells and a deeper network may achieve better results.\n",
    "\n",
    "A dense fully connected layer with 100 neurons connects to the LSTM hidden layers to interpret the features extracted from the sequence. The output layer predicts the next word as a single vector the size of the vocabulary with a probability for each word in the vocabulary. A softmax activation function is used to ensure the outputs have the characteristics of normalized probabilities.\n",
    "\n",
    "__input_length:__ Length of input sequences, when it is constant. This argument is required if you are going to connect Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 50, 75)            768600    \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 50, 200)           140800    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 50, 200)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 50, 200)           240800    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 50, 200)           0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50, 100)           20100     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 50, 10248)         1035048   \n",
      "=================================================================\n",
      "Total params: 2,205,348\n",
      "Trainable params: 2,205,348\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model creation\n",
    "model = Sequential()\n",
    "# input dimensions: vocab_size\n",
    "# output dimensions: vector_size\n",
    "model.add(Embedding(vocab_size, 75, input_length = seq_length))\n",
    "model.add(Bidirectional(LSTM(100, return_sequences = True)))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Bidirectional(LSTM(100, return_sequences = True)))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(vocab_size, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = 128\n",
    "epochs = 100\n",
    "checkpoint = ModelCheckpoint(filepath = r'E:\\Documents\\My Projects\\Text Generation\\Models\\BiLSTM_Language_Generation.hdf5',\n",
    "                            verbose = 1, save_best_only=True)\n",
    "stopping = EarlyStopping(monitor='val_loss',patience=5)\n",
    "\n",
    "callbacks = [learning,checkpoint,stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
